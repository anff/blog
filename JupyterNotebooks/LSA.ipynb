{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#潜在语义模型\" data-toc-modified-id=\"潜在语义模型-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>潜在语义模型</a></span></li><li><span><a href=\"#潜在语义分析（LSA）\" data-toc-modified-id=\"潜在语义分析（LSA）-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>潜在语义分析（LSA）</a></span><ul class=\"toc-item\"><li><span><a href=\"#数据集合\" data-toc-modified-id=\"数据集合-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>数据集合</a></span></li><li><span><a href=\"#分词\" data-toc-modified-id=\"分词-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>分词</a></span></li><li><span><a href=\"#建立结构化数据集\" data-toc-modified-id=\"建立结构化数据集-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>建立结构化数据集</a></span></li><li><span><a href=\"#奇异值分解（SVD）\" data-toc-modified-id=\"奇异值分解（SVD）-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>奇异值分解（SVD）</a></span></li><li><span><a href=\"#非负矩阵分解\" data-toc-modified-id=\"非负矩阵分解-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>非负矩阵分解</a></span></li></ul></li><li><span><a href=\"#概率潜在语义分析(pLSA)\" data-toc-modified-id=\"概率潜在语义分析(pLSA)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>概率潜在语义分析(pLSA)</a></span></li><li><span><a href=\"#潜在狄利克雷分配(LDA)\" data-toc-modified-id=\"潜在狄利克雷分配(LDA)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>潜在狄利克雷分配(LDA)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gibbs抽样\" data-toc-modified-id=\"Gibbs抽样-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Gibbs抽样</a></span><ul class=\"toc-item\"><li><span><a href=\"#初始化\" data-toc-modified-id=\"初始化-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>初始化</a></span></li><li><span><a href=\"#训练\" data-toc-modified-id=\"训练-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>训练</a></span></li></ul></li></ul></li><li><span><a href=\"#20新闻组数据集\" data-toc-modified-id=\"20新闻组数据集-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>20新闻组数据集</a></span><ul class=\"toc-item\"><li><span><a href=\"#下载停止词汇\" data-toc-modified-id=\"下载停止词汇-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>下载停止词汇</a></span></li><li><span><a href=\"#20新闻组语料库\" data-toc-modified-id=\"20新闻组语料库-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>20新闻组语料库</a></span></li><li><span><a href=\"#预处理\" data-toc-modified-id=\"预处理-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>预处理</a></span></li><li><span><a href=\"#结构化数据\" data-toc-modified-id=\"结构化数据-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>结构化数据</a></span></li><li><span><a href=\"#SVD_LSA\" data-toc-modified-id=\"SVD_LSA-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>SVD_LSA</a></span></li><li><span><a href=\"#NMF_LSA\" data-toc-modified-id=\"NMF_LSA-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>NMF_LSA</a></span></li><li><span><a href=\"#pLSA\" data-toc-modified-id=\"pLSA-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>pLSA</a></span></li><li><span><a href=\"#LDA\" data-toc-modified-id=\"LDA-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>LDA</a></span></li></ul></li><li><span><a href=\"#Bakcups\" data-toc-modified-id=\"Bakcups-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Bakcups</a></span><ul class=\"toc-item\"><li><span><a href=\"#随机梯度下降-(Stochastic-gradient-descent)\" data-toc-modified-id=\"随机梯度下降-(Stochastic-gradient-descent)-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>随机梯度下降 (Stochastic gradient descent)</a></span><ul class=\"toc-item\"><li><span><a href=\"#the-pLSA\" data-toc-modified-id=\"the-pLSA-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>the pLSA</a></span></li><li><span><a href=\"#the-LDA\" data-toc-modified-id=\"the-LDA-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>the LDA</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 潜在语义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要参考赵航的书"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "def document_to_terms(doc, stop_words={\"\"}):\n",
    "    \n",
    "    stop_words = set(stop_words)\n",
    "    alphas = string.ascii_letters\n",
    "    doc_terms=[]\n",
    "    term = []\n",
    "    for char in doc:\n",
    "        if char in alphas:\n",
    "            term.append(char)\n",
    "        else:\n",
    "            if len(term) != 0:\n",
    "                word = \"\".join(term)\n",
    "                term=[]\n",
    "                if word.lower() not in stop_words:\n",
    "                    doc_terms.append(word)\n",
    "                    \n",
    "    if len(term) != 0:\n",
    "        word = \"\".join(term)\n",
    "        term=[]\n",
    "        if word.lower() not in stop_words:\n",
    "            doc_terms.append(word)\n",
    "            \n",
    "    return doc_terms\n",
    "\n",
    "\n",
    "class StructuredDocuments:\n",
    "    \n",
    "    def set_documents(self, documents, stop_words=[\"\"], threshold = 0):\n",
    "        \n",
    "        \n",
    "        \n",
    "        terms_count = { }\n",
    "        stop_words_set = set(stop_words)\n",
    "        for doc in documents:\n",
    "            doc_terms = document_to_terms(doc, stop_words=stop_words_set)\n",
    "            \n",
    "            for term in doc_terms:\n",
    "                lower_term = term.lower()\n",
    "                if lower_term not in terms_count:\n",
    "                    terms_count[lower_term] = 0\n",
    "                terms_count[lower_term] += 1\n",
    "        \n",
    "        for term in terms_count:\n",
    "            n = terms_count[term]\n",
    "            if n < threshold:\n",
    "                stop_words_set.add(term)\n",
    "                \n",
    "                \n",
    "        terms = []\n",
    "        terms_set = set()\n",
    "        term_to_index = { }\n",
    "        docs_terms = []\n",
    "        docs_term_indices = []\n",
    "                \n",
    "        \n",
    "        for doc in documents:\n",
    "            doc_terms = document_to_terms(doc, stop_words=stop_words_set)\n",
    "            # numpy array\n",
    "            doc_term_indices = np.zeros(len(doc_terms), dtype=np.int)\n",
    "            \n",
    "            for n, term in enumerate(doc_terms):\n",
    "                lower_term = term.lower()\n",
    "                if lower_term not in terms_set:\n",
    "                    term_to_index[lower_term] = len(terms)\n",
    "                    terms_set.add(lower_term)\n",
    "                    terms.append(lower_term)\n",
    "                doc_term_indices[n] = term_to_index[lower_term]\n",
    "                    \n",
    "            docs_terms.append(doc_terms)            \n",
    "            docs_term_indices.append(doc_term_indices)\n",
    "                    \n",
    "        self.documents_ = documents\n",
    "        self.terms_ = terms\n",
    "        self.terms_set_ = terms_set\n",
    "        self.term_to_index_ = term_to_index\n",
    "        self.docs_terms_ = docs_terms\n",
    "        \n",
    "        self.n_terms = len(terms)\n",
    "        self.n_documents = len(documents)\n",
    "        self.docs_term_indices = docs_term_indices\n",
    "        \n",
    "        self.stop_words_set = stop_words_set\n",
    "    \n",
    "    def print_info(self, n = None, n_shown_terms = 20):\n",
    "        sdocs = self\n",
    "        print(\"%d terms found in %d documents.\"%(len(sdocs.terms_), len(sdocs.documents_)))\n",
    "        \n",
    "        print(\"terms:\", [term for term in itertools.islice(sdocs.terms_, n_shown_terms)])\n",
    "        print(\"terms_set:\", [term for term in itertools.islice(sdocs.terms_set_, n_shown_terms)])\n",
    "        \n",
    "        print(\"term_to_index:\", [(term,sdocs.term_to_index_[term]) for term in itertools.islice(sdocs.term_to_index_, n)])\n",
    "        print(\"docs_terms:\", [ [word for word in itertools.islice(term, n)]\\\n",
    "                                  for term in itertools.islice(sdocs.docs_terms_, n)])\n",
    "        print(\"docs_term_indices:\", [[idx for idx in itertools.islice(term, n)]\\\n",
    "                                for term in itertools.islice(sdocs.docs_term_indices, n)])\n",
    "\n",
    "class LSABase:\n",
    "    \n",
    "    def set_documents(self, documents, stop_words=[\"\"], threshold = 0):\n",
    "        sdocs = StructuredDocuments()\n",
    "        sdocs.set_documents(documents, stop_words=stop_words, threshold = threshold)\n",
    "        self.set_structured_documents(sdocs)\n",
    "        \n",
    "    def set_structured_documents(self, sdocs):\n",
    "            \n",
    "        self.structured_documents = sdocs\n",
    "        self.n_terms = sdocs.n_terms\n",
    "        self.n_documents = sdocs.n_documents\n",
    "        self.docs_term_indices = sdocs.docs_term_indices\n",
    "        \n",
    "        \n",
    "    # set(_structured)_documents first\n",
    "    def TFIDF(self):\n",
    "        # terms-documents matrix\n",
    "        \n",
    "        n_terms = self.n_terms\n",
    "        n_documents = self.n_documents\n",
    "        docs_term_indices = self.docs_term_indices\n",
    "        \n",
    "        tf_ij = np.zeros((n_terms, n_documents))\n",
    "        tf_j = np.zeros(n_documents)  # number of terms in document j \n",
    "        df_i = np.zeros(n_terms)  # number of documents which contain term i\n",
    "        df = n_documents\n",
    "\n",
    "\n",
    "        term_doc_mat = np.zeros((n_terms, n_documents))\n",
    "        for j, doc_term_indices in enumerate(docs_term_indices):\n",
    "            \n",
    "            tf_j[j] = len(doc_term_indices)\n",
    "\n",
    "            add_to_df_i = np.zeros(n_terms)\n",
    "            \n",
    "            for i in doc_term_indices:                \n",
    "                \n",
    "                tf_ij[i][j] += 1\n",
    "                \n",
    "                add_to_df_i[i] = 1\n",
    "\n",
    "            df_i += add_to_df_i\n",
    "\n",
    "        # in case, no show word\n",
    "        IDF = np.log(df/np.maximum(df_i, 1))\n",
    "        # X in hang's book\n",
    "        # in case empty document\n",
    "        term_doc_mat = tf_ij/np.maximum(tf_j, 1)*IDF[:,np.newaxis]\n",
    "        \n",
    "        self.tf_ij = tf_ij\n",
    "        self.tf_j = tf_j\n",
    "        self.df_i = df_i\n",
    "        self.df = df\n",
    "        self.IDF = IDF\n",
    "        self.tfidf = term_doc_mat\n",
    "        self.term_doc_mat = term_doc_mat\n",
    "\n",
    "    \n",
    "    # set(_structured)_documents first\n",
    "    def TF(self):\n",
    "        self.TFIDF()\n",
    "        self.term_doc_mat = self.tf_ij / np.maximum(self.tf_j, 1)     \n",
    "    \n",
    "    def print_mat(self, **kargs):\n",
    "        print_mat(self, **kargs)\n",
    "\n",
    "    def print_doc_doc_similarity(self):\n",
    "        print_doc_doc_similarity(self)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def print_mat(lsa, max_topics=None, max_terms=None, max_docs=None, topic_names=None):\n",
    "    #print_doc_topic_mat(lsa, max_topics=max_topics, topic_names=topic_names)\n",
    "    print_doc_vectors(lsa, max_topics=max_topics, max_docs=max_docs, topic_names=topic_names)\n",
    "    print()\n",
    "    print_topic_vectors(lsa, max_topics=max_topics, max_terms=max_terms, topic_names=topic_names)\n",
    "             \n",
    "def print_topic_vectors(lsa, max_topics = None, max_terms = None, topic_names=None):\n",
    "    if max_topics is None:\n",
    "        max_topics = 20\n",
    "    if max_terms is None:\n",
    "        max_terms = 5\n",
    "    \n",
    "    term_topic_mat = lsa.term_topic_mat\n",
    "\n",
    "    n_terms = lsa.n_terms\n",
    "\n",
    "    terms = lsa.structured_documents.terms_\n",
    "    ntopics = term_topic_mat.shape[1]\n",
    "\n",
    "    ntopics = min(max_topics, ntopics)\n",
    "    n_terms = min(max_terms, n_terms)\n",
    "    \n",
    "    for t in range(ntopics):\n",
    "        if topic_names is None:\n",
    "            tname = \"topic%d\"%t\n",
    "        else:\n",
    "            tname = topic_names[t]\n",
    "            \n",
    "        ts = \"%7s\"%tname\n",
    "        \n",
    "        print(ts, \" = \", end=\"\")\n",
    "        index = np.argsort(-np.abs(term_topic_mat[:,t]))\n",
    "\n",
    "        for j in range(n_terms):\n",
    "            i = index[j]\n",
    "            print(\"%+3f*%s\"%(term_topic_mat[i, t], terms[i]), end=\"\")\n",
    "        print()\n",
    "        \n",
    "def print_doc_vectors(lsa, max_topics = None, max_docs = None, topic_names=None):\n",
    "    if max_docs is None:\n",
    "        max_docs = 10\n",
    "    if max_topics is None:\n",
    "        max_topics = 5\n",
    "    \n",
    "    topic_doc_mat = lsa.topic_doc_mat\n",
    "\n",
    "    n_documents = lsa.n_documents\n",
    "    ntopics = topic_doc_mat.shape[0]\n",
    "\n",
    "    ntopics = min(max_topics, ntopics)\n",
    "    n_documents = min(max_docs, n_documents)\n",
    "    \n",
    "    for d in range(n_documents):\n",
    "                     \n",
    "        ds = \"%7s\"%(\"doc%d\"%d)        \n",
    "        print(ds, \" = \", end=\"\")\n",
    "        \n",
    "        index = np.argsort(-np.abs(topic_doc_mat[:,d]))\n",
    "\n",
    "        for j in range(ntopics):        \n",
    "            t = index[j]         \n",
    "            \n",
    "            if topic_names is None:\n",
    "                tname = \"topic%d\"%t\n",
    "            else:\n",
    "                tname = topic_names[t]\n",
    "            \n",
    "            print(\"%+.1e*%s\"%(topic_doc_mat[t, d], tname), end=\"\")\n",
    "        print()\n",
    "    \n",
    "    \n",
    "def print_doc_topic_mat(lsa, max_topics = None, topic_names=None, max_docs=None):\n",
    "    if max_topics is None:\n",
    "        max_topics = 20\n",
    "    if max_docs is None:\n",
    "        max_docs = 15\n",
    "        \n",
    "    term_topic_mat = lsa.term_topic_mat\n",
    "    topic_doc_mat = lsa.topic_doc_mat\n",
    "\n",
    "    n_documents = lsa.n_documents\n",
    "    n_terms = lsa.n_terms\n",
    "\n",
    "    terms = lsa.structured_documents.terms_\n",
    "    ntopics = topic_doc_mat.shape[0]    \n",
    "    \n",
    "    ntopics = min(ntopics, max_topics)\n",
    "    n_documents = min(n_documents, max_docs)\n",
    "\n",
    "    print(\"%8s\"%\"document\", end=\"\")\n",
    "    for t in range(ntopics):\n",
    "        if topic_names is None:\n",
    "            tname = \"topic%d\"%t\n",
    "        else:\n",
    "            tname = topic_names[t]\n",
    "             \n",
    "        ts = \" %10s\"%tname\n",
    "        \n",
    "        print(ts, end=\"\")\n",
    "    print()\n",
    "\n",
    "    for i in range(n_documents):\n",
    "        print(\"%8d\"%i, end=\"\")\n",
    "        for t in range(ntopics):\n",
    "            print(\" %+.1e\"%topic_doc_mat[t, i], end=\"\")\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "    \n",
    "\n",
    "        \n",
    "class SVD_LSA(LSABase):\n",
    "                        \n",
    "    def topic_decompose(self, ntopics,term_weight = \"TFIDF\"):        \n",
    "        if term_weight == \"TFIDF\":\n",
    "            self.TFIDF()\n",
    "        elif term_weight == \"TF\":\n",
    "            self.TF()\n",
    "            \n",
    "        U, S, VT = np.linalg.svd(self.term_doc_mat, full_matrices=False)\n",
    "        \n",
    "        U = U[:,:ntopics]\n",
    "        VT = VT[:ntopics,:]\n",
    "        S = S[:ntopics]\n",
    "        \n",
    "        self.U = U\n",
    "        self.S = S\n",
    "        self.VT = VT\n",
    "        self.term_topic_mat = U\n",
    "        self.topic_doc_mat = VT\n",
    "        self.ntopics = ntopics\n",
    "\n",
    "# non-negative matrix factorization\n",
    "def NMF(A, k, max_iters, seed = None, on_update=None):\n",
    "    # A = [m x n]\n",
    "    # W = [m x k]\n",
    "    # H = [k x n]\n",
    "    if seed is None:\n",
    "        seed = 0\n",
    "\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    W = rng.rand(m, k)\n",
    "    H = rng.rand(k, n)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        Hold = H\n",
    "        Wold = W\n",
    "        # update imediately\n",
    "        #(k,n) =  (k,n) * { [(k,m) (m,n)] / [(k,m)(m,k)(k,n) + epsilon]}  }\n",
    "        H = H * ((W.T.dot(A)) / (W.T.dot(W).dot(H) + 1E-20))\n",
    "        # update imediately\n",
    "        #(k,n) =  (m,k) * { [(m,n) (n,k)] / [(m,k)(k,n)(n,k)]}\n",
    "        W = W * ((A.dot(H.transpose())) / (W.dot(H.dot(H.transpose())) + 1E-20))\n",
    "        \n",
    "        #print(A)\n",
    "        #print(H)\n",
    "        #print(W)\n",
    "        \n",
    "        #ad-hoc normalization\n",
    "        scaleW = W.sum(axis=0)\n",
    "        W = W/scaleW\n",
    "        H = H*scaleW[:,np.newaxis]\n",
    "        \n",
    "        if on_update is not None:\n",
    "            if on_update(i, H, W, Hold, Wold):\n",
    "                break\n",
    "\n",
    "    return W, H\n",
    "\n",
    "class NMF_LSA(LSABase):\n",
    "\n",
    "    \n",
    "        \n",
    "    def topic_decompose(self, ntopics, term_weight = \"TFIDF\", max_iters=200,  seed = None, on_update=None):        \n",
    "        if term_weight == \"TFIDF\":\n",
    "            self.TFIDF()\n",
    "        elif term_weight == \"TF\":\n",
    "            self.TF()\n",
    "            \n",
    "        n_terms = self.n_terms\n",
    "        n_documents = self.n_documents\n",
    "        term_doc_mat = self.term_doc_mat\n",
    " \n",
    "        W, H = NMF(term_doc_mat, ntopics, max_iters = max_iters, seed=seed, on_update=on_update)\n",
    "\n",
    "        #print(W)\n",
    "        #print(H)\n",
    "        \n",
    "        # ad-hoc normalization\n",
    "        if term_weight == \"TFIDF\":\n",
    "            #print(self.IDF)\n",
    "            #print(1./np.maximum(self.IDF, 1E-20))\n",
    "            scaleW = W.T.dot( 1./np.maximum(self.IDF, 1E-20) )\n",
    "        elif term_weight == \"TF\":\n",
    "            scaleW = W.sum(axis=0)\n",
    "                        \n",
    "        W = W/scaleW\n",
    "        H = H*scaleW[:,np.newaxis]\n",
    "                \n",
    "        #print(W)\n",
    "        #print(H)\n",
    "        \n",
    "        self.term_topic_mat = W\n",
    "        self.topic_doc_mat = H\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "        self.ntopics = ntopics\n",
    "        \n",
    "class pLSA(LSABase):\n",
    "    \n",
    "    def topic_decompose(self, ntopics, max_iters=200, on_update=None, seed=0, alpha=1E-10):\n",
    "\n",
    "        n_terms = self.n_terms\n",
    "        n_documents = self.n_documents\n",
    "        docs_term_indices = self.docs_term_indices\n",
    "        \n",
    "        rng = np.random.RandomState(seed=seed)\n",
    "        Pwz = rng.rand(n_terms, ntopics)\n",
    "        Pzd = rng.rand(ntopics, n_documents)\n",
    "\n",
    "        Pwz = Pwz/Pwz.sum(axis=0)\n",
    "        \n",
    "        nwd = alpha*np.ones((n_terms, n_documents)) # number of word `w` in document `d`\n",
    "\n",
    "        for j, doc_term_indices in enumerate(docs_term_indices):                \n",
    "            for i in doc_term_indices:                                \n",
    "                nwd[i][j] += 1\n",
    "  \n",
    "        for i in range(max_iters):\n",
    "\n",
    "            nd = nwd.sum(axis=0)\n",
    "\n",
    "            # add 1E-6 to avoid 0\n",
    "            # we need to inverse the elements of this matrix\n",
    "            Pwd = Pwz.dot(Pzd) + 1E-20\n",
    "\n",
    "            Pwz_new = (nwd/Pwd).dot(Pzd.transpose()) * Pwz\n",
    "            Pwz_new = Pwz_new / Pwz_new.sum(axis=0)\n",
    "\n",
    "\n",
    "            Pzd_new = (nwd/Pwd).transpose().dot(Pwz).transpose() * Pzd\n",
    "            Pzd_new = Pzd_new / nd\n",
    "\n",
    "            #print(np.power(Pwz_new-Pwz,2).sum())\n",
    "            #print(np.power(Pzd_new-Pzd,2).sum())\n",
    "            #print(np.power(Pzd_new,2).sum())\n",
    "                        \n",
    "            if on_update is not None:\n",
    "                if on_update(i, Pwz_new, Pzd_new, Pwz, Pzd):\n",
    "                    Pwz = Pwz_new\n",
    "                    Pzd = Pzd_new\n",
    "                    break\n",
    "\n",
    "            Pwz = Pwz_new\n",
    "            Pzd = Pzd_new\n",
    "            \n",
    "        self.term_topic_mat = Pwz\n",
    "        self.topic_doc_mat = Pzd\n",
    "        self.ntopics = ntopics\n",
    "        \n",
    "        \n",
    "class LDA(LSABase):\n",
    "        \n",
    "    # intialization randomly\n",
    "    # assign each word a topic\n",
    "    def initialize_randomly(self, ntopics):\n",
    "        n_documents = self.n_documents\n",
    "        n_terms = self.n_terms\n",
    "        \n",
    "        n_mk = np.zeros((n_documents, ntopics))\n",
    "        n_kv = np.zeros((ntopics, n_terms))\n",
    "        n_m = np.zeros(n_documents)\n",
    "        n_k = np.zeros(ntopics)\n",
    "\n",
    "        k_mp = n_documents*[None]\n",
    "        v_mp =  self.docs_term_indices\n",
    "        \n",
    "        #assert type(self.docs_term_indices[0]) is np.ndarray\n",
    "        \n",
    "        for m in range(n_documents):\n",
    "            length = len(v_mp[m])\n",
    "            \n",
    "            # generate len(doc_terms) random topics\n",
    "            k = np.random.randint(ntopics, size=length, dtype=np.int)\n",
    "            \n",
    "            # assign topic\n",
    "            k_mp[m] = k[:]\n",
    "            \n",
    "            # assign document length\n",
    "            n_m[m] = length\n",
    "            \n",
    "            for p in range(length):\n",
    "                v = v_mp[m][p]\n",
    "                n_mk[m, k] += 1 \n",
    "                n_kv[k,v] += 1 \n",
    "                n_k[k] += 1\n",
    "\n",
    "        self.n_mk = n_mk\n",
    "        self.n_kv = n_kv\n",
    "        self.n_m = n_m\n",
    "        self.n_k = n_k\n",
    "        self.k_mp = k_mp\n",
    "        self.v_mp = v_mp\n",
    "\n",
    "    def topic_decompose(self, ntopics, alpha=1, beta=1, warnming_iters=1000, estimate_iters=100, sampling_method=\"Gibbs\"):        \n",
    "        n_documents = self.n_documents\n",
    "        n_terms = self.n_terms\n",
    "        \n",
    "        self.initialize_randomly(ntopics)\n",
    "        \n",
    "        n_mk = self.n_mk\n",
    "        n_m = self.n_m\n",
    "        n_kv = self.n_kv\n",
    "        n_k = self.n_k\n",
    "        \n",
    "        theta_mk_sum = np.zeros((n_documents, ntopics))\n",
    "        phi_kv_sum = np.zeros((ntopics, n_terms))\n",
    "        \n",
    "        for _ in range(warnming_iters):\n",
    "            if sampling_method == \"Gibbs\":\n",
    "                self.Gibbs_step(self.zw_Gibbs_topic_prob, alpha, beta)\n",
    "            else:\n",
    "                self.MH_step(self.zw_MH_topic_prob, alpha, beta)\n",
    "\n",
    "        for _ in range(estimate_iters):\n",
    "            if sampling_method == \"Gibbs\":\n",
    "                self.Gibbs_step(self.zw_Gibbs_topic_prob, alpha, beta)\n",
    "            else:\n",
    "                self.MH_step(self.zw_MH_topic_prob, alpha, beta)\n",
    "\n",
    "            #print(n_mk)\n",
    "            #print(n_kv)\n",
    "            theta_mk_sum += (n_mk + alpha)/(n_mk + alpha).sum(axis=1, keepdims=True)\n",
    "            phi_kv_sum += (n_kv + beta)/(n_kv + beta).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        theta_mk = theta_mk_sum/estimate_iters\n",
    "        phi_kv = phi_kv_sum/estimate_iters\n",
    "        \n",
    "        self.theta_mk = theta_mk\n",
    "        self.phi_kv = phi_kv\n",
    "        self.term_topic_mat = phi_kv.transpose()\n",
    "        self.topic_doc_mat = theta_mk.transpose()\n",
    "        \n",
    "        \n",
    "    # generate P(z|w) sample\n",
    "    def zw_Gibbs_topic_prob(self, m, v, alpha, beta):\n",
    "        n_mk = self.n_mk\n",
    "        n_m = self.n_m\n",
    "        n_kv = self.n_kv\n",
    "        n_k = self.n_k\n",
    "            \n",
    "        n_kv_ = (n_kv[:,v] + beta) / (n_kv[:,:] + beta).sum(axis=1)\n",
    "        n_mk_ = (n_mk[m,:] + alpha) / (n_mk[m,:] + alpha).sum()\n",
    "        return n_kv_*n_mk_\n",
    "\n",
    "    # generate P(z|w) sample\n",
    "    def zw_MH_topic_prob(m, v, k, alpha, beta):\n",
    "        n_mk = self.n_mk\n",
    "        n_m = self.n_m\n",
    "        n_kv = self.n_kv\n",
    "        n_k = self.n_k\n",
    "        \n",
    "        n_kv_ = (n_kv[k,v] + beta) / (n_kv[k,:] + beta).sum()\n",
    "        n_mk_ = (n_mk[m,k] + alpha) / (n_mk[m,:] + alpha).sum()\n",
    "        return n_kv_*n_mk_\n",
    "\n",
    "    \n",
    "    def MH_step(self, topic_prob, alpha, beta):\n",
    "\n",
    "        n_documents = self.n_documents\n",
    "\n",
    "        v_mp = self.v_mp\n",
    "        k_mp = self.k_mp\n",
    "        \n",
    "        n_mk = self.n_mk\n",
    "        n_m = self.n_m\n",
    "        n_kv = self.n_kv\n",
    "        n_k = self.n_k\n",
    "        \n",
    "        # random document\n",
    "        m = np.random.randint(n_document, dtype=np.int)\n",
    "        # random term\n",
    "        p = np.random.randint(len(v_mp[m]), dtype=np.int)\n",
    "        \n",
    "        v = v_mp[m][p]\n",
    "        k = k_mp[m][p]    \n",
    "        \n",
    "        # random new topic\n",
    "        kp = np.random.randint(ntopics, dtype=np.int)\n",
    "\n",
    "        k_prob = topic_prob(m, v, k, alpha, beta)\n",
    "        kp_prob = topic_prob(m, v, kp, alpha, beta)\n",
    "\n",
    "        acc = kp_prob / (k_prob + 1E-10)\n",
    "        u = np.random.rand()\n",
    "        if u < acc:\n",
    "            # accept      \n",
    "            n_mk[m, k] -= 1\n",
    "            #n_m[m] -= 1\n",
    "            n_kv[k, v] -= 1\n",
    "            n_k[k] -= 1\n",
    "\n",
    "            k_mp[m][p] = kp\n",
    "            \n",
    "            n_mk[m, kp] += 1\n",
    "            #n_m[m] += 1\n",
    "            n_kv[kp, v] += 1\n",
    "            n_k[kp] += 1\n",
    "\n",
    "    \n",
    "    \n",
    "    def Gibbs_step(self, sample_topic, alpha, beta):\n",
    "        n_documents = self.n_documents\n",
    "        \n",
    "        v_mp = self.v_mp\n",
    "        k_mp = self.k_mp\n",
    "        \n",
    "        n_mk = self.n_mk\n",
    "        n_m = self.n_m\n",
    "        n_kv = self.n_kv\n",
    "        n_k = self.n_k\n",
    "\n",
    "        for m in range(n_documents):\n",
    "            for p in range(len(v_mp[m])):\n",
    "                \n",
    "                # current term\n",
    "                v = v_mp[m][p]\n",
    "\n",
    "                # current topic\n",
    "                k = k_mp[m][p]   \n",
    "                \n",
    "                n_mk[m, k] -= 1\n",
    "                #n_m[m] -= 1\n",
    "                n_kv[k, v] -= 1\n",
    "                n_k[k] -= 1\n",
    "\n",
    "                topics_prob = sample_topic(m, v, alpha, beta)\n",
    "                acc = topics_prob.cumsum()/topics_prob.sum()\n",
    "                u = np.random.rand()\n",
    "                kp = np.searchsorted(acc, u, side=\"left\")\n",
    "\n",
    "                k_mp[m][p] = kp\n",
    "                \n",
    "                n_mk[m, kp] += 1\n",
    "                #n_m[m] += 1\n",
    "                n_kv[kp, v] += 1\n",
    "                n_k[kp] += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 潜在语义分析（LSA）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的文档包含两个主题：一个是粒子物理，常用词汇为\"meson\", \"photon\"。\n",
    "另外一个是生活，常用词汇为\"girl\", \"boy\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents set\n",
    "documents=[\n",
    "    \"Girl loves boy\",\n",
    "    \"Boy loves girl\",\n",
    "    \"Girl loves girl\",\n",
    "    \"Boy loves boy\",\n",
    "    \n",
    "    \"Meson hits photon\",\n",
    "    \"Photon hits meson\",\n",
    "    \"Photon hits photon\",\n",
    "    \"Meson hits meson\",\n",
    "\n",
    "    \"Boy loves meson\",\n",
    "    \"Girl loves photon\",\n",
    "    \"Meson hits girl\",\n",
    "    \"Photon hits boy a\",\n",
    "    \"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bc', 'bc', 'C']\n"
     ]
    }
   ],
   "source": [
    "print(document_to_terms(\"a Bc bc 1C\", stop_words=[\"a\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立结构化数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 terms found in 13 documents.\n",
      "terms: ['girl', 'loves', 'boy', 'meson', 'hits', 'photon']\n",
      "terms_set: ['boy', 'hits', 'meson', 'loves', 'photon', 'girl']\n",
      "term_to_index: [('girl', 0), ('loves', 1), ('boy', 2), ('meson', 3), ('hits', 4), ('photon', 5)]\n",
      "docs_terms: [['Girl', 'loves', 'boy'], ['Boy', 'loves', 'girl'], ['Girl', 'loves', 'girl'], ['Boy', 'loves', 'boy'], ['Meson', 'hits', 'photon'], ['Photon', 'hits', 'meson'], ['Photon', 'hits', 'photon'], ['Meson', 'hits', 'meson'], ['Boy', 'loves', 'meson'], ['Girl', 'loves', 'photon'], ['Meson', 'hits', 'girl'], ['Photon', 'hits', 'boy'], []]\n",
      "docs_term_indices: [[0, 1, 2], [2, 1, 0], [0, 1, 0], [2, 1, 2], [3, 4, 5], [5, 4, 3], [5, 4, 5], [3, 4, 3], [2, 1, 3], [0, 1, 5], [3, 4, 0], [5, 4, 2], []]\n"
     ]
    }
   ],
   "source": [
    "sdocs = StructuredDocuments()\n",
    "sdocs.set_documents(documents, threshold=2)\n",
    "sdocs.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用频率逆文档(TFIDF)来表示term-document矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term-document (7X13) matrix setup\n",
      "[[1. 1. 2. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 2. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 2. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 2. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 4. 0.]\n",
      "[5. 6. 5. 5. 6. 5. 1.]\n",
      "[[0.319 0.319 0.637 0.    0.    0.    0.    0.    0.    0.319 0.319 0.\n",
      "  0.   ]\n",
      " [0.258 0.258 0.258 0.258 0.    0.    0.    0.    0.258 0.258 0.    0.\n",
      "  0.   ]\n",
      " [0.319 0.319 0.    0.637 0.    0.    0.    0.    0.319 0.    0.    0.239\n",
      "  0.   ]\n",
      " [0.    0.    0.    0.    0.319 0.319 0.    0.637 0.319 0.    0.319 0.\n",
      "  0.   ]\n",
      " [0.    0.    0.    0.    0.258 0.258 0.258 0.258 0.    0.    0.258 0.193\n",
      "  0.   ]\n",
      " [0.    0.    0.    0.    0.319 0.319 0.637 0.    0.    0.319 0.    0.239\n",
      "  0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.641\n",
      "  0.   ]]\n"
     ]
    }
   ],
   "source": [
    "lsa = LSABase()\n",
    "lsa.set_documents(documents)\n",
    "lsa.TFIDF()\n",
    "with np.printoptions(precision=3):\n",
    "    print(\"term-document (%dX%d) matrix setup\"%(lsa.term_doc_mat.shape[0],lsa.term_doc_mat.shape[1]))\n",
    "    print(lsa.tf_ij)\n",
    "    print(lsa.tf_j)\n",
    "    print(lsa.df_i)\n",
    "    print(lsa.tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 奇异值分解（SVD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将term-documents矩阵分解为 u s vT\n",
    "保留其中主值最大的部分。\n",
    "奇异值分解的正交性要求，矩阵的部分数值小于0，如何解释呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc0  = +3.3e-01*topic1-3.1e-01*topic0\n",
      "   doc1  = +3.3e-01*topic1-3.1e-01*topic0\n",
      "   doc2  = +3.2e-01*topic1-3.2e-01*topic0\n",
      "   doc3  = +3.4e-01*topic1-3.0e-01*topic0\n",
      "   doc4  = -3.6e-01*topic1-2.8e-01*topic0\n",
      "   doc5  = -3.6e-01*topic1-2.8e-01*topic0\n",
      "   doc6  = -3.5e-01*topic1-2.7e-01*topic0\n",
      "   doc7  = -3.7e-01*topic1-2.9e-01*topic0\n",
      "   doc8  = -3.0e-01*topic0+1.1e-01*topic1\n",
      "   doc9  = -3.0e-01*topic0+1.1e-01*topic1\n",
      "\n",
      " topic0  = -0.440937*girl-0.437121*loves-0.415396*meson-0.398767*boy-0.377042*hits\n",
      " topic1  = -0.466845*hits+0.430359*loves-0.418304*meson-0.389008*photon+0.381818*boy\n"
     ]
    }
   ],
   "source": [
    "lsa = SVD_LSA()\n",
    "lsa.set_documents(documents)\n",
    "lsa.topic_decompose(ntopics=2, term_weight=\"TF\")\n",
    "lsa.print_mat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非负矩阵分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将term-documents矩阵$X$分解为 $WH$，其中$W$和$H$为（任意元素）非负矩阵。\n",
    "目标为优化\n",
    "$$\\sum_{ij}(WH-X)_{ij}^2\n",
    "优化方法采用“乘法更新规则”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want 2 topics\n",
      "   doc0  = +1.0e+00*physics+3.4e-77*live\n",
      "   doc1  = +1.0e+00*physics+5.5e-77*live\n",
      "   doc2  = +1.0e+00*physics+1.9e-24*live\n",
      "   doc3  = +1.0e+00*physics+0.0e+00*live\n",
      "   doc4  = +1.0e+00*live+4.8e-48*physics\n",
      "   doc5  = +1.0e+00*live+4.4e-48*physics\n",
      "   doc6  = +1.0e+00*live+4.1e-63*physics\n",
      "   doc7  = +1.1e+00*live+1.9e-36*physics\n",
      "   doc8  = +6.9e-01*physics+3.3e-01*live\n",
      "   doc9  = +7.0e-01*physics+3.1e-01*live\n",
      "  doc10  = +7.2e-01*live+3.2e-01*physics\n",
      "  doc11  = +5.2e-01*live+2.4e-01*physics\n",
      "  doc12  = +0.0e+00*live+0.0e+00*physics\n",
      "\n",
      "   live  = +0.330953*hits+0.328633*meson+0.300137*photon+0.023426*a+0.016849*girl\n",
      "physics  = +0.342109*loves+0.317835*girl+0.308588*boy+0.014114*meson+0.009756*photon\n"
     ]
    }
   ],
   "source": [
    "ntopics = 2\n",
    "print(\"We want %d topics\"%ntopics)\n",
    "\n",
    "lsa = NMF_LSA()\n",
    "lsa.set_documents(documents)\n",
    "lsa.topic_decompose(ntopics=2, term_weight=\"TF\")\n",
    "lsa.print_mat(topic_names=[\"live\",\"physics\"], max_docs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want 2 topics\n",
      "   doc0  = +1.0e+00*physics+8.7e-105*live\n",
      "   doc1  = +1.0e+00*physics+1.5e-104*live\n",
      "   doc2  = +1.0e+00*physics+4.5e-60*live\n",
      "   doc3  = +1.0e+00*physics+1.1e-304*live\n",
      "   doc4  = +1.0e+00*live+1.6e-76*physics\n",
      "   doc5  = +1.0e+00*live+1.5e-76*physics\n",
      "   doc6  = +1.0e+00*live+2.3e-140*physics\n",
      "   doc7  = +1.0e+00*live+2.4e-42*physics\n",
      "   doc8  = +6.4e-01*physics+3.7e-01*live\n",
      "   doc9  = +6.4e-01*physics+3.6e-01*live\n",
      "  doc10  = +6.4e-01*live+3.8e-01*physics\n",
      "  doc11  = +6.5e-01*live+3.3e-01*physics\n",
      "  doc12  = +0.0e+00*live+0.0e+00*physics\n",
      "\n",
      "   live  = +0.309315*meson+0.296353*photon+0.255493*hits+0.074902*a+0.006163*girl\n",
      "physics  = +0.310531*girl+0.300454*boy+0.263659*loves+0.027415*a+0.006760*meson\n"
     ]
    }
   ],
   "source": [
    "ntopics = 2\n",
    "print(\"We want %d topics\"%ntopics)\n",
    "\n",
    "lsa = NMF_LSA()\n",
    "lsa.set_documents(documents)\n",
    "lsa.topic_decompose(ntopics=2, term_weight=\"TFIDF\")\n",
    "lsa.print_mat(topic_names=[\"live\",\"physics\"], max_docs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概率潜在语义分析(pLSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概率潜在语义分析即潜在语义分析概率化版本，是一种生成模型。\n",
    "\n",
    "设文档集合为$D=\\{d_j\\}, j < |D|$，单词集合为$W=\\{w_i\\}, i < |W|$，文档$d_j$也是单词的序列，序列长度为$|d_j|$（重复单词多次计数。）\n",
    "\n",
    "概率模型给出文档$d_j$中某一单词位置（比如第一个单词），单词$w_i$出现的概率$P(w_i, d_j)$为\n",
    "$$\n",
    "P(w_i, d_j) = \n",
    "\\sum_k P(w_i, z_k) P(z_k, d_j)\n",
    "$$\n",
    "其中$P(z_k, d_j)$为文档$d_j$中某一单词选取话题$z_k$的概率，$P(w_i, z_k)$为话题$z_k$中单词$w_i$出现的概率。\n",
    "\n",
    "设文档$d_j$第$i^\\prime$个单词是单词$w_{i(d_j, i^\\prime)}$（$i^\\prime < |d_j|$），\n",
    "则文档$d_j$生成的概率为\n",
    "$$\n",
    "\\prod_{i^\\prime < |d_j|} P(w_{i(d_j, i^\\prime)}, d_j)\n",
    "=\\prod_{i < |W|} { P(w_i, d_j)^{n(w_i, d_j)} }\n",
    "$$\n",
    "$n(w_i, d_j)$为单词$i$在文档$j$中出现的频次。\n",
    "所有文档的生成概率的对数为\n",
    "$$\n",
    "L = \\sum_j \\sum_{i} n(w_i, d_j) \\log P(w_i, d_j)\n",
    "$$\n",
    "优化方法采用EM算法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want 2 topics\n",
      "   doc0  = +1.0e+00*physics+2.1e-34*live\n",
      "   doc1  = +1.0e+00*physics+4.5e-34*live\n",
      "   doc2  = +1.0e+00*physics+5.2e-46*live\n",
      "   doc3  = +1.0e+00*physics+5.2e-30*live\n",
      "   doc4  = +1.0e+00*live+1.7e-30*physics\n",
      "   doc5  = +1.0e+00*live+9.7e-31*physics\n",
      "   doc6  = +1.0e+00*live+2.3e-35*physics\n",
      "   doc7  = +1.0e+00*live+9.3e-30*physics\n",
      "   doc8  = +6.7e-01*physics+3.3e-01*live\n",
      "   doc9  = +6.7e-01*physics+3.3e-01*live\n",
      "  doc10  = +6.7e-01*live+3.3e-01*physics\n",
      "  doc11  = +7.5e-01*live+2.5e-01*physics\n",
      "  doc12  = +5.7e-01*live+4.3e-01*physics\n",
      "\n",
      "   live  = +0.315789*hits+0.315789*photon+0.315789*meson+0.052632*a+0.000000*boy\n",
      "physics  = +0.333333*loves+0.333333*girl+0.333333*boy+0.000000*meson+0.000000*photon\n"
     ]
    }
   ],
   "source": [
    "ntopics = 2\n",
    "print(\"We want %d topics\"%ntopics)\n",
    "\n",
    "lsa = pLSA()\n",
    "lsa.set_documents(documents)\n",
    "lsa.topic_decompose(ntopics=ntopics)\n",
    "lsa.print_mat(topic_names=[\"live\",\"physics\"], max_docs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 潜在狄利克雷分配(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设文档集合为$D=\\{d_m\\}, m<|D|$，单词集合为$W=\\{w_i\\},i < |W|$，文档$d_j$也是单词的序列，序列长度为$|d_m|$（重复单词多次计数。）。\n",
    "话题向量为$\\mathbf{z}=\\{z_m\\},m < |D|$。\n",
    "单词向量为$\\mathbf{w} = \\{w_{mn}\\}, m < |D|, n< |d_m|$。\n",
    "话题向量为$\\mathbf{z} = \\{z_{mn}\\}, m < |D|, n< |d_m|$。\n",
    "话题向量先验计数为$\\mathbf{\\alpha} = \\{\\alpha_{mk}\\}, m < |D|, k < K$。\n",
    "话题词向量先验计数为$\\mathbf{\\beta} = \\{\\beta_{ki}\\}, k < K, i < |W|$。\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\mathbf{w}, \\mathbf{z}, \\mathbf{\\theta},\\mathbf{\\phi}| \\alpha, \\beta) = \n",
    "\\left[\\prod_k \\text{Dir} (\\phi_k|\\beta)\\right]\n",
    "\\left[\\prod_m \\text{Dir}(\\theta_m|\\alpha)\\right]\n",
    "\\left[\n",
    "\\prod_{mn} \\text{Cat}(z_{mn}|\\theta_m) \n",
    "\\text{Cat}(w_{mn}|\\phi_{z_{mn}})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "我们想要求 $P(\\mathbf{z},\\theta,\\phi|\\mathbf{w})$\n",
    "$$P(\\mathbf{z},\\theta,\\phi|\\mathbf{w})\n",
    "=\n",
    "P(\\theta,\\phi|\\mathbf{z},\\mathbf{w})\n",
    "P(\\mathbf{z},\\mathbf{w})\n",
    "/P(\\mathbf{w})\n",
    "=\n",
    "P(\\theta,\\phi|\\mathbf{z},\\mathbf{w})\n",
    "P(\\mathbf{z}|\\mathbf{w})\n",
    "$$\n",
    "其中\n",
    "\n",
    "$$\n",
    "P(\\mathbf{z},\\mathbf{w}) = \\prod_{k} \\frac{B(n_k+\\beta)}{B(\\beta)} \\prod_{m} \\frac{B(n_m+\\alpha)}{B(\\alpha)}\n",
    "$$\n",
    "\n",
    "和\n",
    "\n",
    "$$\n",
    "P(\\theta,\\phi|\\mathbf{z},\\mathbf{w})\n",
    "= \\left[\\prod_m \\text{Dir}(\\theta_m|n_m+\\alpha)\\right] \\left[\\prod_k \\text{Dir}(\\phi_k|n_k+\\beta)\\right]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs抽样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\mathbf{z}|\\mathbf{w})$可以通过Gibbs抽样方法得到。\n",
    "注意根据狄利克雷分布，\n",
    "$\\theta_m$ 的期望为 $$(n_m + \\alpha)/\\sum_k (n_{mk}+\\alpha_{mk})$$\n",
    "$\\phi_k$ 的期望为 $$(n_k + \\beta)/\\sum_{i} (n_{ki}+\\beta_{ki})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want 2 topics\n",
      "[[3. 3.]\n",
      " [3. 3.]\n",
      " [3. 3.]\n",
      " [0. 3.]\n",
      " [3. 0.]\n",
      " [3. 3.]\n",
      " [3. 3.]\n",
      " [3. 3.]\n",
      " [0. 3.]\n",
      " [3. 0.]\n",
      " [3. 3.]\n",
      " [4. 4.]\n",
      " [0. 0.]]\n",
      "[3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 4. 0.]\n",
      "[[6. 4. 3. 5. 6. 6. 1.]\n",
      " [5. 5. 6. 5. 5. 4. 1.]]\n",
      "[31. 31.]\n",
      "[array([0, 1, 2]), array([2, 1, 0]), array([0, 1, 0]), array([2, 1, 2]), array([3, 4, 5]), array([5, 4, 3]), array([5, 4, 5]), array([3, 4, 3]), array([2, 1, 3]), array([0, 1, 5]), array([3, 4, 0]), array([5, 4, 2, 6]), array([], dtype=int32)]\n",
      "[array([1, 0, 0]), array([0, 1, 1]), array([1, 0, 1]), array([1, 1, 1]), array([0, 0, 0]), array([1, 0, 1]), array([0, 1, 1]), array([1, 0, 0]), array([1, 1, 1]), array([0, 0, 0]), array([0, 0, 1]), array([1, 0, 0, 1]), array([], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "ntopics=2\n",
    "print(\"We want %d topics\"%ntopics)\n",
    "lda=LDA()\n",
    "lda.set_documents(documents)\n",
    "lda.initialize_randomly(ntopics=ntopics)\n",
    "print(lda.n_mk)\n",
    "print(lda.n_m)\n",
    "print(lda.n_kv)\n",
    "print(lda.n_k)\n",
    "print(lda.v_mp)\n",
    "print(lda.k_mp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want 2 topics\n",
      "   doc0  = +5.7e-01*topic0+4.3e-01*topic1\n",
      "   doc1  = +6.1e-01*topic1+3.9e-01*topic0\n",
      "   doc2  = +6.8e-01*topic1+3.2e-01*topic0\n",
      "   doc3  = +6.4e-01*topic0+3.6e-01*topic1\n",
      "   doc4  = +6.0e-01*topic0+4.0e-01*topic1\n",
      "   doc5  = +6.2e-01*topic0+3.8e-01*topic1\n",
      "   doc6  = +6.4e-01*topic1+3.6e-01*topic0\n",
      "   doc7  = +7.0e-01*topic0+3.0e-01*topic1\n",
      "   doc8  = +5.7e-01*topic0+4.3e-01*topic1\n",
      "   doc9  = +6.9e-01*topic1+3.1e-01*topic0\n",
      "\n",
      " topic0  = +0.266818*hits+0.205121*meson+0.186861*boy+0.160548*loves+0.075752*girl\n",
      " topic1  = +0.308973*photon+0.246785*girl+0.155302*loves+0.127163*boy+0.084368*meson\n"
     ]
    }
   ],
   "source": [
    "ntopics=2\n",
    "print(\"We want %d topics\"%ntopics)\n",
    "lda=LDA()\n",
    "lda.set_documents(documents)\n",
    "lda.topic_decompose(ntopics = ntopics, alpha = 1, beta = 1)\n",
    "lda.print_mat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20新闻组数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载停止词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'would']\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    # downaload stop words\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "else:\n",
    "    # I download for you, and save it for you\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "stop_words.extend(string.ascii_letters)\n",
    "stop_words.extend([\"would\"])\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20新闻组语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语料库取自 http://qwone.com/~jason/20Newsgroups/ 。\n",
    "你也可以在sklearn中找到这个数据集。\n",
    "在 https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json 包含json格式的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#wget 'https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json' -O newsgroups.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>From: a207706@moe.dseg.ti.com (Robert Loper)\\n...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>From: kimman@magnus.acs.ohio-state.edu (Kim Ri...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>Subject: Re: Don't more innocents die without ...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>From: livesey@solntze.wpd.sgi.com (Jon Livesey...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>From: dls@aeg.dsto.gov.au (David Silver)\\nSubj...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>Subject: Re: Mike Francesa's 1993 Predictions\\...</td>\n",
       "      <td>9</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>From: jet@netcom.Netcom.COM (J. Eric Townsend)...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>From: gld@cunixb.cc.columbia.edu (Gary L Dare)...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>From: sehari@iastate.edu (Babak Sehari)\\nSubje...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>From: danmg@grok85.ColumbiaSC.NCR.COM (Daniel ...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>From: henry@zoo.toronto.edu (Henry Spencer)\\nS...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>From: tzs@stein2.u.washington.edu (Tim Smith)\\...</td>\n",
       "      <td>18</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>From: U56149@uicvm.uic.edu\\nSubject: LCIII &amp; M...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>From: nsmca@aurora.alaska.edu\\nSubject: Lunar ...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  target  \\\n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "10     From: irwin@cmptrc.lonestar.org (Irwin Arnstei...       8   \n",
       "100    From: tchen@magnus.acs.ohio-state.edu (Tsung-K...       6   \n",
       "1000   From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...       2   \n",
       "10000  From: a207706@moe.dseg.ti.com (Robert Loper)\\n...       7   \n",
       "10001  From: kimman@magnus.acs.ohio-state.edu (Kim Ri...       6   \n",
       "10002  From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...       2   \n",
       "10003  Subject: Re: Don't more innocents die without ...       0   \n",
       "10004  From: livesey@solntze.wpd.sgi.com (Jon Livesey...       0   \n",
       "10005  From: dls@aeg.dsto.gov.au (David Silver)\\nSubj...       1   \n",
       "10006  Subject: Re: Mike Francesa's 1993 Predictions\\...       9   \n",
       "10007  From: jet@netcom.Netcom.COM (J. Eric Townsend)...       8   \n",
       "10008  From: gld@cunixb.cc.columbia.edu (Gary L Dare)...      10   \n",
       "10009  From: sehari@iastate.edu (Babak Sehari)\\nSubje...      12   \n",
       "1001   From: danmg@grok85.ColumbiaSC.NCR.COM (Daniel ...       7   \n",
       "10010  From: henry@zoo.toronto.edu (Henry Spencer)\\nS...      14   \n",
       "10011  From: tzs@stein2.u.washington.edu (Tim Smith)\\...      18   \n",
       "10012  From: U56149@uicvm.uic.edu\\nSubject: LCIII & M...       4   \n",
       "10013  From: nsmca@aurora.alaska.edu\\nSubject: Lunar ...      14   \n",
       "\n",
       "                  target_names  \n",
       "0                    rec.autos  \n",
       "1        comp.sys.mac.hardware  \n",
       "10             rec.motorcycles  \n",
       "100               misc.forsale  \n",
       "1000   comp.os.ms-windows.misc  \n",
       "10000                rec.autos  \n",
       "10001             misc.forsale  \n",
       "10002  comp.os.ms-windows.misc  \n",
       "10003              alt.atheism  \n",
       "10004              alt.atheism  \n",
       "10005            comp.graphics  \n",
       "10006       rec.sport.baseball  \n",
       "10007          rec.motorcycles  \n",
       "10008         rec.sport.hockey  \n",
       "10009          sci.electronics  \n",
       "1001                 rec.autos  \n",
       "10010                sci.space  \n",
       "10011       talk.politics.misc  \n",
       "10012    comp.sys.mac.hardware  \n",
       "10013                sci.space  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_json('./newsgroups.json')\n",
    "\n",
    "\n",
    "#df = df.head(1000)\n",
    "#display(df)\n",
    "\n",
    "email_documents = df.content.values.tolist()\n",
    "target_names = df.target_names.values.tolist()\n",
    "targets = df.target.values.tolist()\n",
    "\n",
    "n_catogories = len(df.target.unique())\n",
    "catogories = range(n_catogories)\n",
    "cat_names = n_catogories*[None]\n",
    "\n",
    "for i, t in enumerate(targets):\n",
    "    cat_names[t] = target_names[i]\n",
    "print(cat_names) \n",
    "\n",
    "#print(email_documents[10])\n",
    "#print(target_names[10])\n",
    "\n",
    "display(df.head(n=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理\n",
    "sklearn提供了这个数据集的预处理选项，但是我们自己做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'''''From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'''''\n",
      "''''' I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'''''\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "shown=0\n",
    "print(\"'''''%s'''''\"%email_documents[shown])\n",
    "\n",
    "# remove email header\n",
    "email_documents = [\"\\n\\n\".join(doc.split('\\n\\n')[1:]) for doc in email_documents]\n",
    "\n",
    "# Remove Emails\n",
    "email_documents = [re.sub('\\S*@\\S*\\s?', '', doc) for doc in email_documents]\n",
    "\n",
    "print(\"'''''%s'''''\"%email_documents[shown])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结构化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23417 terms found in 11314 documents.\n",
      "terms: ['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'door', 'sports', 'looked', 'late', 'early', 'called', 'doors', 'really', 'small', 'addition', 'front', 'bumper', 'separate']\n",
      "terms_set: ['gravitational', 'handlebars', 'exug', 'username', 'install', 'settling', 'ornaments', 'soldered', 'ernest', 'jonathan', 'suffix', 'archbishop', 'hardcover', 'swung', 'glossy', 'jubilee', 'poly', 'nz', 'elastic', 'modify']\n",
      "term_to_index: [('wondering', 0), ('anyone', 1), ('could', 2), ('enlighten', 3), ('car', 4), ('saw', 5), ('day', 6), ('door', 7), ('sports', 8), ('looked', 9)]\n",
      "docs_terms: [['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'door', 'sports', 'car'], ['fair', 'number', 'brave', 'souls', 'upgraded', 'SI', 'clock', 'oscillator', 'shared', 'experiences'], ['line', 'Ducati', 'GTS', 'model', 'clock', 'Runs', 'well', 'paint', 'bronze', 'brown'], ['Software', 'publishing', 'windows', 'OCR', 'System', 'Windows', 'OCR', 'System', 'DOS', 'Unregistered'], ['Anybody', 'seen', 'mouse', 'cursor', 'distortion', 'running', 'Diamond', 'driver', 'Sorry', 'know'], ['article', 'James', 'Callison', 'writes', 'article', 'David', 'Hwang', 'writes', 'article', 'wharfie'], ['New', 'slightly', 'used', 'SyQuest', 'cartridge', 'forsale', 'Asking', 'shipping', 'included', 'Please'], ['charge', 'purchasing', 'computer', 'software', 'small', 'office', 'question', 'Microsoft', 'Office', 'Pack'], ['article', 'James', 'Tims', 'writes', 'maintaining', 'classes', 'even', 'prison', 'seems', 'place'], ['article', 'Frank', 'Dwyer', 'writes', 'article', 'Jon', 'Livesey', 'writes', 'article', 'Frank']]\n",
      "docs_term_indices: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 4], [41, 42, 43, 44, 45, 46, 47, 48, 49, 50], [88, 89, 90, 23, 47, 91, 92, 93, 94, 95], [133, 134, 135, 136, 137, 135, 136, 137, 138, 139], [171, 172, 173, 174, 175, 176, 177, 178, 179, 22], [195, 196, 197, 198, 195, 199, 200, 198, 195, 201], [289, 290, 262, 291, 292, 293, 294, 295, 296, 35], [299, 300, 301, 133, 15, 302, 303, 152, 302, 304], [195, 196, 341, 198, 342, 343, 344, 345, 323, 346], [195, 389, 390, 198, 195, 391, 392, 198, 195, 389]]\n"
     ]
    }
   ],
   "source": [
    "email_sdocs=StructuredDocuments()\n",
    "email_sdocs.set_documents(email_documents, stop_words=stop_words, threshold=5)\n",
    "email_sdocs.print_info(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD_LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_topics=50\n",
    "svd=True\n",
    "if svd:\n",
    "    svd_lsa=SVD_LSA()\n",
    "    svd_lsa.set_structured_documents(email_sdocs)\n",
    "    svd_lsa.topic_decompose(ntopics=N_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc0  = -1.5e-02*topic43-1.4e-02*topic39-1.2e-02*topic35+1.2e-02*topic30-1.1e-02*topic11-1.0e-02*topic42-1.0e-02*topic38-1.0e-02*topic4+1.0e-02*topic12-9.9e-03*topic29+9.0e-03*topic3+8.9e-03*topic44+8.9e-03*topic41-8.5e-03*topic27+8.2e-03*topic48+7.4e-03*topic22-7.2e-03*topic21-7.0e-03*topic34+7.0e-03*topic36+6.2e-03*topic14+6.0e-03*topic26+5.3e-03*topic17-5.0e-03*topic19-4.7e-03*topic45+4.6e-03*topic33+4.1e-03*topic18-2.8e-03*topic13+2.7e-03*topic28-2.6e-03*topic15-2.5e-03*topic47+2.1e-03*topic46-2.1e-03*topic23+1.9e-03*topic32-1.8e-03*topic31+1.5e-03*topic10+1.5e-03*topic40+1.2e-03*topic20+8.0e-04*topic37-7.7e-04*topic49-6.9e-04*topic25+5.6e-04*topic16+4.4e-04*topic5+3.8e-04*topic7-3.7e-04*topic9-3.7e-04*topic8+3.1e-04*topic1-1.4e-04*topic6+1.2e-04*topic24-2.7e-05*topic2-5.0e-06*topic0\n",
      "   doc1  = -8.9e-03*topic42-8.2e-03*topic48+7.6e-03*topic45+7.0e-03*topic14+6.6e-03*topic3-6.5e-03*topic4-6.4e-03*topic40+5.1e-03*topic49+4.7e-03*topic31+4.5e-03*topic12-4.5e-03*topic9-4.5e-03*topic27+4.4e-03*topic22-4.3e-03*topic30-4.2e-03*topic34-3.9e-03*topic44-3.4e-03*topic33+2.7e-03*topic11-2.4e-03*topic38-2.3e-03*topic32+2.3e-03*topic43-2.2e-03*topic37-2.0e-03*topic17+1.6e-03*topic10-1.5e-03*topic35-1.5e-03*topic26+1.5e-03*topic21-1.3e-03*topic28-1.3e-03*topic18-1.3e-03*topic20+1.1e-03*topic47+1.1e-03*topic16-8.5e-04*topic23+8.1e-04*topic25-7.0e-04*topic29+6.9e-04*topic24+6.8e-04*topic36-6.6e-04*topic19+4.9e-04*topic39+4.8e-04*topic15+3.5e-04*topic7-3.1e-04*topic8+2.9e-04*topic5-2.6e-04*topic46+2.5e-04*topic1+1.7e-04*topic13-1.4e-04*topic6-2.0e-05*topic2+1.9e-05*topic41-4.1e-06*topic0\n",
      "   doc2  = -1.4e-02*topic43+1.2e-02*topic28+1.0e-02*topic44+9.8e-03*topic12-9.7e-03*topic42-8.7e-03*topic39+7.8e-03*topic41+7.6e-03*topic30-7.0e-03*topic4+6.6e-03*topic22-6.4e-03*topic27+6.3e-03*topic19+6.3e-03*topic48+6.2e-03*topic3+5.7e-03*topic47+4.3e-03*topic49-4.2e-03*topic35-4.2e-03*topic38+3.8e-03*topic16+3.4e-03*topic14-3.2e-03*topic40+2.9e-03*topic32-2.8e-03*topic21-2.5e-03*topic15+2.4e-03*topic46-2.4e-03*topic11+2.3e-03*topic20-1.7e-03*topic13+1.7e-03*topic26-1.6e-03*topic23-1.4e-03*topic33+1.2e-03*topic10-1.1e-03*topic17+1.1e-03*topic31+7.8e-04*topic36-6.3e-04*topic34+5.3e-04*topic29-5.3e-04*topic9-4.2e-04*topic45-3.8e-04*topic18+3.3e-04*topic24-3.0e-04*topic37+2.6e-04*topic7+2.5e-04*topic5+2.1e-04*topic1-1.3e-04*topic25-1.2e-04*topic8-1.1e-04*topic6-1.8e-05*topic2-7.4e-06*topic0\n",
      "   doc3  = -4.1e-02*topic12-4.1e-02*topic14-2.8e-02*topic9-2.6e-02*topic17-1.8e-02*topic48+1.8e-02*topic31-1.7e-02*topic22-1.7e-02*topic46-1.6e-02*topic28-1.5e-02*topic27+1.3e-02*topic49-1.2e-02*topic34-1.2e-02*topic4-1.1e-02*topic44+1.1e-02*topic26+9.7e-03*topic3-8.9e-03*topic35-8.3e-03*topic32-7.8e-03*topic45+7.7e-03*topic36-7.5e-03*topic33+7.5e-03*topic41-7.2e-03*topic43-6.8e-03*topic11-6.2e-03*topic21-5.8e-03*topic38+5.8e-03*topic16+5.7e-03*topic19+4.7e-03*topic20+4.1e-03*topic24-3.9e-03*topic42-3.4e-03*topic18+3.3e-03*topic30-2.9e-03*topic25-2.8e-03*topic39+2.7e-03*topic29+2.4e-03*topic37+2.1e-03*topic23+2.0e-03*topic13+2.0e-03*topic40-9.8e-04*topic8+9.6e-04*topic15+9.6e-04*topic7-6.7e-04*topic10+5.4e-04*topic47+5.2e-04*topic5-3.4e-04*topic6+3.4e-04*topic1-2.8e-05*topic2-3.6e-06*topic0\n",
      "   doc4  = +3.1e-02*topic21+2.2e-02*topic30-2.1e-02*topic49+1.7e-02*topic48-1.6e-02*topic9+1.6e-02*topic33-1.5e-02*topic38-1.5e-02*topic12+1.3e-02*topic32+1.3e-02*topic44-1.2e-02*topic16+1.2e-02*topic29-1.2e-02*topic31-1.1e-02*topic4+1.1e-02*topic46+1.1e-02*topic19+9.5e-03*topic3-9.0e-03*topic14-8.3e-03*topic18+7.2e-03*topic36+6.4e-03*topic43+5.5e-03*topic41-4.6e-03*topic20+4.4e-03*topic24+4.0e-03*topic39+3.9e-03*topic22-3.2e-03*topic27-3.0e-03*topic28-3.0e-03*topic17+3.0e-03*topic47-2.9e-03*topic11+2.8e-03*topic45+2.6e-03*topic15+2.5e-03*topic23+2.5e-03*topic26+2.0e-03*topic37-1.6e-03*topic13-1.2e-03*topic35-9.8e-04*topic8+8.6e-04*topic42+8.3e-04*topic25+7.0e-04*topic7+5.4e-04*topic10+5.1e-04*topic34+4.5e-04*topic5-3.9e-04*topic40+2.8e-04*topic1-2.4e-04*topic6-3.5e-05*topic2-4.3e-06*topic0\n",
      "   doc5  = +7.6e-03*topic30-6.2e-03*topic4+5.5e-03*topic11-5.3e-03*topic38+5.3e-03*topic3+4.9e-03*topic28-4.7e-03*topic20+4.5e-03*topic44+4.4e-03*topic9+3.6e-03*topic32-3.5e-03*topic39+3.4e-03*topic41-3.0e-03*topic42+2.9e-03*topic45-2.6e-03*topic35+2.3e-03*topic36+2.1e-03*topic33+1.8e-03*topic18-1.7e-03*topic49-1.5e-03*topic43-1.4e-03*topic27-1.2e-03*topic29+1.1e-03*topic48-1.0e-03*topic40-1.0e-03*topic15+9.7e-04*topic10-9.7e-04*topic31-9.4e-04*topic34+8.8e-04*topic16-8.7e-04*topic19+6.8e-04*topic14-6.8e-04*topic12-6.7e-04*topic37+5.6e-04*topic22-5.4e-04*topic24+3.8e-04*topic13+3.1e-04*topic46+2.2e-04*topic5-2.2e-04*topic21-2.0e-04*topic17+1.5e-04*topic1+1.4e-04*topic26-1.1e-04*topic47+1.1e-04*topic25+9.3e-05*topic7-7.5e-05*topic6-4.1e-05*topic8-1.8e-05*topic2+1.1e-05*topic23-5.0e-06*topic0\n",
      "   doc6  = +7.7e-02*topic12-6.1e-02*topic14-5.4e-02*topic15-4.1e-02*topic19+3.1e-02*topic27+2.9e-02*topic11-1.9e-02*topic9-1.8e-02*topic28-1.5e-02*topic34+1.5e-02*topic46-1.5e-02*topic26-1.5e-02*topic22-1.3e-02*topic30-1.2e-02*topic4+1.2e-02*topic36+1.0e-02*topic3-9.2e-03*topic17+8.4e-03*topic49+8.1e-03*topic16+7.4e-03*topic37-6.7e-03*topic32+6.0e-03*topic35-5.8e-03*topic29-5.8e-03*topic24+5.7e-03*topic47+5.3e-03*topic44+4.7e-03*topic23+4.3e-03*topic41+4.0e-03*topic10-3.7e-03*topic39+3.0e-03*topic25-2.5e-03*topic40-2.4e-03*topic20+2.4e-03*topic38+2.2e-03*topic18+2.1e-03*topic43-1.8e-03*topic13+1.3e-03*topic21+1.1e-03*topic48+9.2e-04*topic42+8.7e-04*topic7+7.1e-04*topic31+5.2e-04*topic33+4.4e-04*topic1+4.3e-04*topic45-2.9e-04*topic6-1.2e-04*topic5-1.2e-04*topic8-4.9e-05*topic2-1.5e-05*topic0\n",
      "   doc7  = -1.6e-02*topic33+8.5e-03*topic29+8.4e-03*topic34-8.3e-03*topic4-7.6e-03*topic36-7.5e-03*topic27+7.3e-03*topic3-5.6e-03*topic48-5.3e-03*topic19+5.3e-03*topic16-4.7e-03*topic31+4.6e-03*topic39-4.2e-03*topic11-3.9e-03*topic9+3.9e-03*topic18+3.7e-03*topic47+3.7e-03*topic32-3.5e-03*topic28-3.4e-03*topic15+3.2e-03*topic22+3.2e-03*topic42-3.2e-03*topic13-2.9e-03*topic46-2.6e-03*topic21+2.3e-03*topic45-1.9e-03*topic38+1.9e-03*topic26-1.8e-03*topic41-1.7e-03*topic10-1.6e-03*topic35+1.6e-03*topic30-1.2e-03*topic37+9.3e-04*topic12+6.0e-04*topic14-5.9e-04*topic23-5.5e-04*topic43+4.2e-04*topic24-3.9e-04*topic20+3.7e-04*topic7+3.6e-04*topic5-3.2e-04*topic8-2.9e-04*topic44-2.4e-04*topic40+2.3e-04*topic1-1.8e-04*topic49-1.6e-04*topic6+4.4e-05*topic17-3.4e-05*topic25-2.0e-05*topic2-3.9e-06*topic0\n",
      "   doc8  = +1.0e-02*topic35+1.0e-02*topic9+7.4e-03*topic36-6.7e-03*topic37-5.7e-03*topic46-5.1e-03*topic4-4.7e-03*topic18+4.4e-03*topic45+4.2e-03*topic3-3.4e-03*topic49+3.2e-03*topic41-3.0e-03*topic12-2.8e-03*topic19-2.7e-03*topic33+2.7e-03*topic20+2.7e-03*topic17-2.3e-03*topic43-2.3e-03*topic44+2.1e-03*topic42+2.0e-03*topic28+1.8e-03*topic31+1.7e-03*topic21-1.7e-03*topic15-1.6e-03*topic22+1.5e-03*topic40+1.5e-03*topic16+1.4e-03*topic30+1.3e-03*topic38-1.1e-03*topic34+1.1e-03*topic23-1.1e-03*topic39+1.1e-03*topic27+9.2e-04*topic29+8.6e-04*topic14+7.3e-04*topic10+7.2e-04*topic24+6.9e-04*topic25-6.9e-04*topic47+5.6e-04*topic48+5.2e-04*topic13-3.9e-04*topic26+3.1e-04*topic32+2.3e-04*topic5-2.2e-04*topic7-1.4e-04*topic11+1.3e-04*topic1-5.6e-05*topic8-4.7e-05*topic6-1.2e-05*topic2-3.5e-06*topic0\n",
      "   doc9  = +1.0e-02*topic9+1.0e-02*topic41+6.4e-03*topic39+6.0e-03*topic17-5.7e-03*topic18+5.7e-03*topic40-5.7e-03*topic42-5.4e-03*topic4+5.1e-03*topic33-5.0e-03*topic44-5.0e-03*topic22+5.0e-03*topic29-4.9e-03*topic45+4.5e-03*topic3+4.5e-03*topic16-4.3e-03*topic30-4.2e-03*topic49-4.1e-03*topic28+4.1e-03*topic35+4.0e-03*topic20-3.8e-03*topic36+3.5e-03*topic43+3.4e-03*topic21+3.4e-03*topic47-2.9e-03*topic15-2.8e-03*topic12+2.5e-03*topic23-2.3e-03*topic19+2.1e-03*topic14-2.0e-03*topic34-1.6e-03*topic46-1.2e-03*topic27+9.1e-04*topic24+7.8e-04*topic10+5.9e-04*topic26+5.8e-04*topic32+4.9e-04*topic11-4.6e-04*topic38-3.2e-04*topic25+3.2e-04*topic13+2.6e-04*topic5-2.5e-04*topic37-2.3e-04*topic7+2.1e-04*topic31-1.7e-04*topic48+1.4e-04*topic1-4.3e-05*topic6-1.5e-05*topic2-1.0e-05*topic8-2.3e-06*topic0\n",
      "\n",
      " topic0  = -0.998232*ax-0.052851*max-0.010601*pl-0.010051*di-0.007640*wm\n",
      " topic1  = +0.998495*subscribe+0.030290*quit+0.027344*comp+0.023195*graphics+0.006648*thanks\n",
      " topic2  = -0.999992*narrative-0.001589*commandment-0.000720*film-0.000617*refrained-0.000554*literary\n",
      " topic3  = +0.745013*test+0.151696*thanks+0.080042*please+0.079678*anyone+0.076893*windows\n",
      " topic4  = +0.661551*test-0.106587*thanks-0.091023*please-0.090557*windows-0.090034*anyone\n",
      " topic5  = -0.998963*bo-0.010290*duke-0.009852*ide-0.009664*shipping-0.009597*meg\n",
      " topic6  = +0.999726*kidding+0.009731*pens+0.005380*caps+0.005038*loose+0.004994*geoff\n",
      " topic7  = -0.998036*satan-0.026389*god-0.014309*crush-0.013411*lord-0.011998*grace\n",
      " topic8  = +0.998238*art+0.014637*productions+0.014594*rap+0.014389*riddle+0.013241*ep\n",
      " topic9  = -0.288918*thanks-0.211933*windows-0.173740*card+0.147849*god-0.133041*advance\n",
      "topic10  = -0.382136*proline-0.382136*alphalpha-0.382136*cosmo-0.377000*angmar-0.323309*bu\n",
      "topic11  = +0.384311*drive-0.362384*thanks+0.247181*scsi+0.206973*shipping+0.191629*mb\n",
      "topic12  = +0.319920*shipping-0.290811*windows+0.225471*email+0.218472*sale+0.197157*please\n",
      "topic13  = +0.716280*testing+0.585830*hello+0.182892*bye-0.150577*thanks+0.128986*flames\n",
      "topic14  = -0.387286*shipping+0.292748*drive+0.280429*thanks-0.263347*windows+0.239386*scsi\n",
      "topic15  = +0.512825*offline+0.508365*contact+0.366349*rick-0.327480*shipping+0.201043*wanted\n",
      "topic16  = -0.398003*monitor-0.269689*vga-0.260706*card+0.239107*drive-0.226206*maine\n",
      "topic17  = -0.382104*maine+0.311510*monitor+0.211729*vga-0.179076*drive-0.174385*windows\n",
      "topic18  = -0.505290*maine-0.221143*lake-0.215175*finals-0.182095*please+0.177664*thanks\n",
      "topic19  = +0.491610*replies+0.491058*email-0.288268*shipping-0.282825*maine-0.243285*thanks\n",
      "topic20  = -0.814725*david+0.187091*god+0.119894*drive-0.108967*pneumonia-0.099593*key\n",
      "topic21  = +0.305563*card-0.297750*monitor-0.284095*drive+0.189192*god+0.164922*replies\n",
      "topic22  = -0.432808*david-0.326846*god-0.219664*monitor-0.214645*drive-0.156102*windows\n",
      "topic23  = -0.572245*pearl-0.505274*nf-0.424419*mt-0.403767*adam+0.119787*god\n",
      "topic24  = +0.621868*pneumonia+0.431827*treating+0.409779*fm+0.333250*effective+0.163745*cubs\n",
      "topic25  = -0.795831*diving-0.561729*prevented-0.123675*one+0.088580*cubs+0.073365*suck\n",
      "topic26  = -0.535942*cubs-0.441940*suck-0.256327*harkey-0.144754*shipping+0.141238*sale\n",
      "topic27  = -0.345627*sale+0.281290*shipping-0.226967*cubs-0.196906*suck+0.182634*included\n",
      "topic28  = +0.420333*window-0.259205*mail+0.206475*replies-0.194436*address-0.179716*com\n",
      "topic29  = +0.362147*update+0.321821*com+0.245335*mouse+0.205259*irq+0.168968*replies\n",
      "topic30  = -0.332914*window-0.223733*god-0.179934*scsi+0.173630*toshiba+0.170323*replies\n",
      "topic31  = -0.278072*georgia-0.220857*toshiba-0.200442*combo+0.194389*dos-0.184479*com\n",
      "topic32  = -0.496587*toshiba-0.469473*combo-0.425819*pas-0.242249*problems+0.167904*drive\n",
      "topic33  = -0.475934*georgia-0.200239*athens+0.186731*drive-0.170223*update-0.167727*mark\n",
      "topic34  = +0.350745*fairchild+0.320734*trademark+0.300561*semiconductor+0.270252*georgia+0.256218*clipper\n",
      "topic35  = +0.264608*bob-0.263216*dolven-0.244974*doug-0.200360*heard+0.198106*kingman\n",
      "topic36  = +0.249782*bob+0.227779*window+0.197198*fairchild+0.188588*kingman+0.179982*trademark\n",
      "topic37  = -0.253248*dolven-0.241316*doug-0.207636*bob+0.167395*gordon-0.164374*heard\n",
      "topic38  = +0.177277*bob+0.168130*says-0.164536*card+0.161834*luck-0.150735*israel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic39  = +0.215626*dolven+0.213485*doug-0.211959*luck+0.195658*update+0.181144*frank\n",
      "topic40  = +0.579089*andreas+0.246576*looking+0.215149*frank+0.186492*system+0.180335*subroutine\n",
      "topic41  = +0.433805*luck+0.358644*frank+0.276527*system+0.251730*good-0.206991*andreas\n",
      "topic42  = +0.471582*luck+0.257240*good-0.216490*frank+0.215495*israel+0.198146*scsi\n",
      "topic43  = -0.348439*scsi+0.235920*hp+0.215427*drive+0.193563*xman+0.180738*version\n",
      "topic44  = -0.273211*israel+0.202215*update+0.199538*scsi-0.194714*simms-0.174045*birthday\n",
      "topic45  = +0.265043*update+0.236029*andreas+0.233589*address-0.192630*title-0.179516*frank\n",
      "topic46  = +0.293209*title+0.275721*lee-0.256636*graphics+0.252949*says-0.200686*kingman\n",
      "topic47  = -0.304404*key+0.304332*graphics+0.224441*comp+0.191617*xman+0.183213*hp\n",
      "topic48  = -0.336108*graphics-0.222208*comp+0.191105*xman+0.175928*go-0.169615*title\n",
      "topic49  = +0.348572*go+0.214125*graphics+0.176310*israel-0.154063*steve-0.151818*barber\n",
      "( 0)                   alt.atheism topic09(9.9e-03) topic03(4.9e-03) topic17(4.5e-03)\n",
      "( 1)                 comp.graphics topic03(7.4e-03) topic47(6.3e-03) topic27(4.1e-03)\n",
      "( 2)       comp.os.ms-windows.misc topic03(7.9e-03) topic30(3.6e-03) topic46(2.4e-03)\n",
      "( 3)      comp.sys.ibm.pc.hardware topic11(1.1e-02) topic03(8.3e-03) topic14(7.8e-03)\n",
      "( 4)         comp.sys.mac.hardware topic03(8.9e-03) topic11(7.6e-03) topic38(6.2e-03)\n",
      "( 5)                comp.windows.x topic28(1.0e-02) topic03(6.9e-03) topic27(4.9e-03)\n",
      "( 6)                  misc.forsale topic12(1.6e-02) topic11(9.7e-03) topic03(7.6e-03)\n",
      "( 7)                     rec.autos topic30(6.3e-03) topic03(6.0e-03) topic09(4.8e-03)\n",
      "( 8)               rec.motorcycles topic30(7.8e-03) topic44(7.0e-03) topic03(6.0e-03)\n",
      "( 9)            rec.sport.baseball topic18(1.1e-02) topic09(8.1e-03) topic03(5.4e-03)\n",
      "(10)              rec.sport.hockey topic18(1.2e-02) topic09(9.0e-03) topic15(7.2e-03)\n",
      "(11)                     sci.crypt topic31(7.9e-03) topic34(7.2e-03) topic22(7.0e-03)\n",
      "(12)               sci.electronics topic03(6.4e-03) topic22(5.2e-03) topic17(1.6e-03)\n",
      "(13)                       sci.med topic37(8.8e-03) topic39(7.5e-03) topic38(7.2e-03)\n",
      "(14)                     sci.space topic03(5.4e-03) topic09(4.7e-03) topic22(3.1e-03)\n",
      "(15)        soc.religion.christian topic09(9.5e-03) topic17(6.0e-03) topic20(6.0e-03)\n",
      "(16)            talk.politics.guns topic09(7.9e-03) topic03(4.7e-03) topic30(3.5e-03)\n",
      "(17)         talk.politics.mideast topic42(9.1e-03) topic09(8.0e-03) topic35(7.4e-03)\n",
      "(18)            talk.politics.misc topic09(7.5e-03) topic03(4.7e-03) topic35(2.4e-03)\n",
      "(19)            talk.religion.misc topic09(9.2e-03) topic03(4.9e-03) topic17(4.4e-03)\n",
      "         topic0               alt.atheism(-4.2e-06)    soc.religion.christian(-4.2e-06)        talk.politics.misc(-4.4e-06)\n",
      "         topic1            comp.windows.x(6.8e-04)             comp.graphics(6.8e-04)        rec.sport.baseball(4.5e-04)\n",
      "         topic2        talk.politics.guns(-1.5e-05)                   sci.med(-1.5e-05)          rec.sport.hockey(-1.6e-05)\n",
      "         topic3     comp.sys.mac.hardware(8.3e-03)  comp.sys.ibm.pc.hardware(8.3e-03)   comp.os.ms-windows.misc(7.9e-03)\n",
      "         topic4     talk.politics.mideast(-5.4e-03)                   sci.med(-5.4e-03)        talk.politics.misc(-5.5e-03)\n",
      "         topic5             comp.graphics(3.3e-04)            comp.windows.x(3.3e-04)           sci.electronics(2.9e-04)\n",
      "         topic6     talk.politics.mideast(2.5e-04)          rec.sport.hockey(2.5e-04)                   sci.med(1.4e-04)\n",
      "         topic7  comp.sys.ibm.pc.hardware(4.9e-04)   comp.os.ms-windows.misc(4.9e-04)              misc.forsale(4.2e-04)\n",
      "         topic8     comp.sys.mac.hardware(6.9e-04)        rec.sport.baseball(6.9e-04)              misc.forsale(2.6e-04)\n",
      "         topic9               alt.atheism(9.5e-03)    soc.religion.christian(9.5e-03)        talk.religion.misc(9.2e-03)\n",
      "        topic10  comp.sys.ibm.pc.hardware(1.1e-03)              misc.forsale(1.1e-03)     comp.sys.mac.hardware(1.1e-03)\n",
      "        topic11  comp.sys.ibm.pc.hardware(9.7e-03)              misc.forsale(9.7e-03)     comp.sys.mac.hardware(7.6e-03)\n",
      "        topic12              misc.forsale(5.5e-03)          rec.sport.hockey(5.5e-03)        rec.sport.baseball(3.7e-03)\n",
      "        topic13  comp.sys.ibm.pc.hardware(9.2e-04)                 rec.autos(9.2e-04)        talk.religion.misc(6.1e-04)\n",
      "        topic14  comp.sys.ibm.pc.hardware(5.5e-03)     comp.sys.mac.hardware(5.5e-03)        rec.sport.baseball(2.1e-03)\n",
      "        topic15          rec.sport.hockey(4.1e-03)        rec.sport.baseball(4.1e-03)  comp.sys.ibm.pc.hardware(2.4e-03)\n",
      "        topic16    soc.religion.christian(2.9e-03)               alt.atheism(2.9e-03)        talk.religion.misc(2.7e-03)\n",
      "        topic17    soc.religion.christian(4.5e-03)               alt.atheism(4.5e-03)        talk.religion.misc(4.4e-03)\n",
      "        topic18          rec.sport.hockey(1.1e-02)        rec.sport.baseball(1.1e-02)                 rec.autos(1.7e-03)\n",
      "        topic19              misc.forsale(1.9e-03)        rec.sport.baseball(1.9e-03)          rec.sport.hockey(1.5e-03)\n",
      "        topic20    soc.religion.christian(3.9e-03)               alt.atheism(3.9e-03)        talk.religion.misc(2.2e-03)\n",
      "        topic21  comp.sys.ibm.pc.hardware(5.6e-03)    soc.religion.christian(5.6e-03)               alt.atheism(3.9e-03)\n",
      "        topic22                 sci.crypt(5.2e-03)           sci.electronics(5.2e-03)                 rec.autos(4.7e-03)\n",
      "        topic23    soc.religion.christian(2.3e-03)               alt.atheism(2.3e-03)        talk.religion.misc(2.3e-03)\n",
      "        topic24                   sci.med(2.1e-03)    soc.religion.christian(2.1e-03)               alt.atheism(1.3e-03)\n",
      "        topic25                 sci.crypt(6.8e-04)            comp.windows.x(6.8e-04)     comp.sys.mac.hardware(5.9e-04)\n",
      "        topic26          rec.sport.hockey(4.2e-03)              misc.forsale(4.2e-03)    soc.religion.christian(2.3e-03)\n",
      "        topic27          rec.sport.hockey(4.9e-03)            comp.windows.x(4.9e-03)             comp.graphics(4.1e-03)\n",
      "        topic28            comp.windows.x(4.8e-03)           rec.motorcycles(4.8e-03)                 rec.autos(4.5e-03)\n",
      "        topic29          rec.sport.hockey(1.8e-03)  comp.sys.ibm.pc.hardware(1.8e-03)     comp.sys.mac.hardware(1.6e-03)\n",
      "        topic30           rec.motorcycles(6.6e-03)                   sci.med(6.6e-03)                 rec.autos(6.3e-03)\n",
      "        topic31                 sci.crypt(3.0e-03)     talk.politics.mideast(3.0e-03)        talk.politics.guns(2.5e-03)\n",
      "        topic32           rec.motorcycles(2.0e-03)                 rec.autos(2.0e-03)                   sci.med(1.5e-03)\n",
      "        topic33                 sci.crypt(2.6e-03)               alt.atheism(2.6e-03)        talk.religion.misc(2.4e-03)\n",
      "        topic34                 sci.crypt(2.4e-03)             comp.graphics(2.4e-03)          rec.sport.hockey(2.2e-03)\n",
      "        topic35     talk.politics.mideast(2.4e-03)        talk.politics.misc(2.4e-03)        rec.sport.baseball(2.0e-03)\n",
      "        topic36        rec.sport.baseball(2.9e-03)            comp.windows.x(2.9e-03)                   sci.med(2.6e-03)\n",
      "        topic37                   sci.med(4.4e-03)          rec.sport.hockey(4.4e-03)  comp.sys.ibm.pc.hardware(1.7e-03)\n",
      "        topic38                   sci.med(6.2e-03)     comp.sys.mac.hardware(6.2e-03)             comp.graphics(3.2e-03)\n",
      "        topic39                   sci.med(3.4e-03)            comp.windows.x(3.4e-03)              misc.forsale(1.9e-03)\n",
      "        topic40             comp.graphics(1.9e-03)        rec.sport.baseball(1.9e-03)        talk.religion.misc(1.8e-03)\n",
      "        topic41           rec.motorcycles(4.8e-03)                 rec.autos(4.8e-03)               alt.atheism(3.0e-03)\n",
      "        topic42     talk.politics.mideast(3.4e-03)             comp.graphics(3.4e-03)            comp.windows.x(2.2e-03)\n",
      "        topic43                 sci.crypt(4.3e-03)     comp.sys.mac.hardware(4.3e-03)             comp.graphics(2.7e-03)\n",
      "        topic44           rec.motorcycles(4.2e-03)                 rec.autos(4.2e-03)                 sci.crypt(4.1e-03)\n",
      "        topic45     comp.sys.mac.hardware(2.9e-03)            comp.windows.x(2.9e-03)                 sci.space(1.9e-03)\n",
      "        topic46     talk.politics.mideast(2.4e-03)   comp.os.ms-windows.misc(2.4e-03)     comp.sys.mac.hardware(1.9e-03)\n",
      "        topic47             comp.graphics(4.6e-03)     talk.politics.mideast(4.6e-03)           rec.motorcycles(4.3e-03)\n",
      "        topic48     talk.politics.mideast(2.5e-03)                 rec.autos(2.5e-03)           rec.motorcycles(2.5e-03)\n",
      "        topic49                 sci.crypt(4.0e-03)     talk.politics.mideast(4.0e-03)           rec.motorcycles(3.1e-03)\n"
     ]
    }
   ],
   "source": [
    "def print_lsa(lsa):\n",
    "    \n",
    "    topic_weights=np.zeros((n_catogories, lsa.ntopics))\n",
    "    for cat in range(n_catogories):\n",
    "        #print(nmf_lsa.topic_doc_mat[:,np.equal(targets, cat+1)])\n",
    "        target = catogories[cat]        \n",
    "        topic_weight = lsa.topic_doc_mat[:,np.equal(targets, target)].mean(axis=1)\n",
    "        topic_weights[cat,:] = topic_weight[:]\n",
    "        \n",
    "        index = np.argsort(-topic_weight)\n",
    "        print(\"(%2d)%30s topic%02d(%.1e) topic%02d(%.1e) topic%02d(%.1e)\"%\\\n",
    "              (target, \"%s\"%(cat_names[target]), index[0], topic_weight[index[0]], index[1],topic_weight[index[1]], index[2], topic_weight[index[2]]   ) )\n",
    "\n",
    "    for topic in range(lsa.ntopics):\n",
    "        cats_weights = topic_weights[:,topic]\n",
    "        cats_weights = cats_weights\n",
    "        index = np.argsort(-cats_weights)\n",
    "        cat0 = index[0]\n",
    "        cat1 = index[1]\n",
    "        cat2 = index[2]\n",
    "        print(\"%15s %25s(%.1e) %25s(%.1e) %25s(%.1e)\"%\\\n",
    "              (\"topic%d\"%topic,\n",
    "               cat_names[cat0], cats_weights[cat1], \\\n",
    "               cat_names[cat1], cats_weights[cat1],\\\n",
    "               cat_names[cat2], cats_weights[cat2] ) )\n",
    "\n",
    "    #print(targets[:10])\n",
    "if svd:\n",
    "    svd_lsa.print_mat(max_topics=100)\n",
    "    print_lsa(svd_lsa)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF_LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want 50 topics\n",
      "[    0] dH 390635.7417046184\n",
      "[    0] dW 131670.00633145001\n",
      "[   10] dH 0.03638915386063681\n",
      "[   10] dW 1711.6659096919473\n",
      "[   20] dH 0.005138609272494576\n",
      "[   20] dW 324.7977598088766\n",
      "[   30] dH 0.001649956745203241\n",
      "[   30] dW 43.71345372077325\n",
      "[   40] dH 0.0007219224249530767\n",
      "[   40] dW 25.80647424711721\n",
      "[   50] dH 0.0008466947759816882\n",
      "[   50] dW 10.054346958888729\n",
      "[   60] dH 0.0012924378149829251\n",
      "[   60] dW 15.608590911959931\n",
      "[   70] dH 0.00112271097045587\n",
      "[   70] dW 2.291987281516485\n",
      "[   80] dH 0.00011165490260669117\n",
      "[   80] dW 1.989792427565592\n",
      "[   90] dH 8.181907620457356e-05\n",
      "[   90] dW 1.3495021407496701\n",
      "[  100] dH 0.0002073055769586335\n",
      "[  100] dW 2.3270420434329404\n",
      "[  110] dH 0.00048241347558258666\n",
      "[  110] dW 1.9268805983085175\n",
      "[  120] dH 0.0003356295011990144\n",
      "[  120] dW 3.6280756411343136\n",
      "[  130] dH 0.0003835967681604186\n",
      "[  130] dW 0.7016947752416887\n",
      "[  140] dH 0.00019912333698075444\n",
      "[  140] dW 0.3947426422453327\n",
      "[  150] dH 3.535898832286113e-05\n",
      "[  150] dW 0.37277736528043376\n",
      "[  160] dH 4.5995192417130375e-05\n",
      "[  160] dW 0.4734989731142349\n",
      "[  170] dH 7.091109579104041e-05\n",
      "[  170] dW 0.36313676598039146\n",
      "[  180] dH 1.2208139881631161e-05\n",
      "[  180] dW 0.22809445785334212\n",
      "[  190] dH 1.2665014215261753e-05\n",
      "[  190] dW 0.2799672623205814\n"
     ]
    }
   ],
   "source": [
    "#N_topics = 3+0*min(3, n_catogories)\n",
    "print(\"We want %d topics\"%N_topics)\n",
    "\n",
    "nmf_lsa = NMF_LSA()\n",
    "nmf_lsa.set_structured_documents(email_sdocs)\n",
    "def on_update(i, W, H, Wold, Hold):\n",
    "    if i % 10 == 0:\n",
    "        dH = H - Hold\n",
    "        dW = W - Wold\n",
    "        print(\"[%5d] dH\"%i, np.einsum(\"ij,ij\", dH, dH))\n",
    "        print(\"[%5d] dW\"%i, np.einsum(\"ij,ij\", dW, dW))\n",
    "    \n",
    "nmf_lsa.topic_decompose(ntopics=N_topics, max_iters=200, on_update=on_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  target  \\\n",
       "0     From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1     From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "10    From: irwin@cmptrc.lonestar.org (Irwin Arnstei...       8   \n",
       "100   From: tchen@magnus.acs.ohio-state.edu (Tsung-K...       6   \n",
       "1000  From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...       2   \n",
       "\n",
       "                 target_names  \n",
       "0                   rec.autos  \n",
       "1       comp.sys.mac.hardware  \n",
       "10            rec.motorcycles  \n",
       "100              misc.forsale  \n",
       "1000  comp.os.ms-windows.misc  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc0  = +1.1e+00*topic0+2.3e-01*topic18+6.8e-02*topic46+6.1e-02*topic8+1.4e-02*topic41+1.1e-02*topic12+9.4e-03*topic25+6.7e-03*topic39+2.0e-03*topic9+1.5e-03*topic3+8.0e-04*topic6+6.6e-05*topic30+3.6e-05*topic4+3.2e-05*topic48+1.5e-07*topic36+8.1e-08*topic24+6.5e-11*topic10+1.7e-20*topic31+3.9e-22*topic14+1.2e-23*topic22+9.0e-27*topic7+7.4e-30*topic45+2.1e-36*topic33+7.9e-37*topic37+2.7e-43*topic2+1.8e-51*topic17+1.7e-58*topic23+1.5e-59*topic20+4.0e-61*topic43+6.3e-62*topic27+6.9e-68*topic15+2.6e-69*topic47+9.7e-74*topic29+7.8e-75*topic1+4.5e-78*topic21+2.3e-81*topic16+2.8e-82*topic5+5.0e-87*topic49+2.0e-92*topic28+3.1e-107*topic35+5.6e-111*topic11+3.0e-126*topic26+5.4e-143*topic38+3.1e-145*topic13+2.2e-177*topic34+2.0e-281*topic32+0.0e+00*topic40+0.0e+00*topic42+0.0e+00*topic44+0.0e+00*topic19\n",
      "   doc1  = +2.4e-01*topic0+2.2e-01*topic35+1.4e-01*topic18+5.4e-02*topic5+3.7e-02*topic43+3.1e-02*topic7+3.0e-02*topic8+2.1e-02*topic37+1.7e-02*topic22+1.2e-02*topic14+9.6e-03*topic9+8.1e-03*topic21+2.7e-03*topic6+2.5e-03*topic48+1.9e-04*topic20+1.6e-04*topic41+2.6e-05*topic39+8.0e-09*topic45+5.5e-11*topic15+3.0e-12*topic36+5.6e-16*topic26+1.7e-18*topic34+3.3e-20*topic10+2.4e-25*topic16+1.9e-26*topic33+5.9e-31*topic17+1.7e-31*topic1+7.2e-34*topic23+7.3e-37*topic2+1.2e-39*topic28+2.5e-41*topic29+3.8e-48*topic27+3.4e-50*topic47+1.2e-54*topic3+4.5e-64*topic12+3.3e-70*topic32+6.1e-75*topic31+4.4e-81*topic11+5.8e-86*topic24+5.6e-93*topic25+1.1e-105*topic30+1.7e-112*topic49+1.5e-115*topic42+2.7e-117*topic46+8.6e-173*topic38+2.9e-179*topic13+4.5e-184*topic4+1.9e-272*topic44+0.0e+00*topic40+0.0e+00*topic19\n",
      "   doc2  = +7.3e-01*topic0+1.1e-01*topic18+6.6e-02*topic8+6.4e-02*topic14+3.2e-02*topic35+1.6e-02*topic43+1.4e-02*topic15+2.3e-03*topic9+1.5e-03*topic45+1.3e-03*topic44+1.2e-03*topic6+9.5e-05*topic33+7.9e-05*topic7+3.7e-05*topic42+2.5e-05*topic31+3.7e-07*topic17+9.6e-09*topic4+8.1e-20*topic2+5.2e-20*topic10+3.5e-23*topic1+9.0e-24*topic30+1.1e-24*topic16+4.2e-25*topic37+1.2e-26*topic21+1.1e-27*topic24+2.9e-28*topic48+1.9e-29*topic41+1.4e-31*topic22+4.4e-32*topic36+9.8e-37*topic39+3.0e-43*topic12+6.8e-45*topic28+8.7e-59*topic27+2.6e-61*topic29+1.4e-65*topic49+1.0e-70*topic32+6.9e-81*topic40+2.1e-92*topic23+7.9e-95*topic47+8.1e-97*topic25+3.9e-104*topic20+1.8e-108*topic26+5.4e-110*topic46+1.4e-143*topic3+9.7e-144*topic5+1.2e-146*topic13+4.9e-172*topic19+4.3e-185*topic11+7.0e-219*topic34+4.0e-244*topic38\n",
      "   doc3  = +1.0e+00*topic22+8.0e-02*topic37+5.7e-02*topic16+3.6e-02*topic15+2.1e-02*topic8+8.6e-04*topic47+1.9e-04*topic33+9.0e-05*topic29+1.8e-05*topic11+5.3e-06*topic12+5.5e-12*topic18+1.9e-13*topic45+3.2e-19*topic21+1.5e-21*topic39+1.2e-29*topic5+6.6e-35*topic43+1.3e-35*topic6+4.8e-42*topic28+2.3e-49*topic20+1.4e-55*topic27+4.1e-57*topic14+4.4e-58*topic36+1.5e-60*topic35+6.5e-66*topic4+2.7e-76*topic3+5.0e-80*topic30+1.8e-90*topic0+7.1e-93*topic41+4.7e-106*topic10+7.4e-119*topic26+5.3e-119*topic25+5.8e-121*topic31+1.6e-141*topic2+2.6e-144*topic49+4.8e-167*topic9+6.1e-180*topic1+3.4e-181*topic7+5.8e-184*topic46+9.5e-185*topic32+8.1e-241*topic13+2.3e-269*topic24+1.9e-274*topic23+2.3e-294*topic17+0.0e+00*topic34+0.0e+00*topic48+0.0e+00*topic19+0.0e+00*topic38+0.0e+00*topic40+0.0e+00*topic42+0.0e+00*topic44\n",
      "   doc4  = +4.3e-01*topic39+2.0e-01*topic28+1.3e-01*topic22+9.5e-02*topic14+8.2e-02*topic5+7.9e-02*topic16+6.8e-02*topic12+6.2e-02*topic46+1.5e-02*topic9+1.5e-02*topic15+1.0e-02*topic3+5.4e-03*topic18+5.3e-03*topic0+6.5e-04*topic30+4.2e-04*topic36+7.6e-06*topic26+5.7e-07*topic17+4.6e-10*topic24+1.7e-39*topic35+5.8e-49*topic7+4.4e-52*topic37+1.4e-53*topic10+6.4e-57*topic1+1.2e-61*topic27+3.0e-65*topic4+1.3e-72*topic34+8.7e-76*topic41+6.6e-88*topic45+1.7e-89*topic8+6.1e-92*topic21+6.2e-94*topic43+2.4e-94*topic20+2.2e-94*topic6+4.8e-98*topic33+3.1e-104*topic23+9.8e-112*topic49+4.3e-123*topic2+6.5e-167*topic25+3.5e-172*topic32+7.1e-180*topic29+2.2e-185*topic31+1.8e-225*topic48+4.3e-262*topic47+0.0e+00*topic38+0.0e+00*topic40+0.0e+00*topic42+0.0e+00*topic44+0.0e+00*topic19+0.0e+00*topic13+0.0e+00*topic11\n",
      "\n",
      " topic0  = +0.009474*people+0.008383*article+0.008164*writes+0.008059*one+0.007503*think\n",
      " topic1  = +0.298331*title+0.258338*says+0.254653*lee+0.084469*steve+0.061777*barber\n",
      " topic2  = +0.279798*bob+0.234544*kingman+0.183867*jewish+0.172176*dave+0.087681*hose\n",
      " topic3  = +0.288906*dolven+0.273550*doug+0.179314*heard+0.165823*mel+0.144713*yankees\n",
      " topic4  = +4.962597*bo+0.043300*greg+0.041034*meg+0.039627*western+0.038721*digital\n",
      " topic5  = +0.298169*bmug+0.229901*message+0.155590*bbs+0.116615*posting+0.111843*views\n",
      " topic6  = +0.895023*testing+0.698869*hello+0.231971*bye+0.163922*flames+0.088971*please\n",
      " topic7  = +0.054061*game+0.039245*team+0.034918*games+0.026897*go+0.023391*win\n",
      " topic8  = +0.157534*sale+0.074761*offer+0.074267*condition+0.057110*asking+0.053387*new\n",
      " topic9  = +0.759586*maine+0.332435*finals+0.326827*lake+0.207414*post+0.184329*state\n",
      "topic10  = +0.661917*toshiba+0.624952*combo+0.565050*pas+0.317812*problems+0.061938*cd\n",
      "topic11  = +4.549450*subscribe+0.135190*quit+0.115326*comp+0.087458*graphics+0.015547*mailing\n",
      "topic12  = +0.087894*file+0.074657*ftp+0.070316*files+0.044876*program+0.039066*graphics\n",
      "topic13  = +0.464005*buggy+0.389492*reader+0.337704*generated+0.295764*news+0.194241*probably\n",
      "topic14  = +0.393733*replies+0.388417*email+0.067405*address+0.062168*please+0.056786*kevin\n",
      "topic15  = +0.187526*window+0.045625*manager+0.041102*xsizehints+0.037774*motif+0.031368*application\n",
      "topic16  = +0.351937*frank+0.268411*system+0.074412*objective+0.051245*morality+0.042885*keith\n",
      "topic17  = +0.614960*cubs+0.468034*suck+0.296719*harkey+0.158220*shawn+0.120774*room\n",
      "topic18  = +0.218598*thanks+0.088365*advance+0.068522*update+0.054913*mail+0.053472*please\n",
      "topic19  = +4.742277*ax+0.208605*max+0.050899*pl+0.043477*wm+0.030002*tm\n",
      "topic20  = +0.749390*pneumonia+0.519530*treating+0.488861*fm+0.396967*effective+0.354630*diving\n",
      "topic21  = +0.198463*israel+0.128454*birthday+0.087617*happy+0.078216*th+0.068624*israeli\n",
      "topic22  = +0.196248*windows+0.162679*dos+0.082576*version+0.052937*ms+0.043164*hp\n",
      "topic23  = +3.391260*satan+0.044740*crush+0.035099*angels+0.034339*grace+0.033323*lord\n",
      "topic24  = +4.366597*kidding+0.040363*pens+0.022481*caps+0.021764*loose+0.021478*geoff\n",
      "topic25  = +0.715570*andreas+0.219854*subroutine+0.218489*beatles+0.195141*stint+0.185319*looking\n",
      "topic26  = +7.213007*narrative+0.011428*commandment+0.005047*film+0.004440*refrained+0.003985*literary\n",
      "topic27  = +0.281945*georgia+0.118379*athens+0.088458*programs+0.084250*university+0.077862*mark\n",
      "topic28  = +0.207806*com+0.144241*mouse+0.141377*irq+0.073947*port+0.072986*modem\n",
      "topic29  = +1.154894*pearl+1.019701*nf+0.857259*mt+0.803766*adam+0.029379*xlib\n",
      "topic30  = +0.756717*fairchild+0.690976*trademark+0.647121*semiconductor+0.504866*andy+0.449831*clipper\n",
      "topic31  = +2.588699*test+0.044591*thanks+0.017939*loopback+0.012314*connector+0.010702*wallace\n",
      "topic32  = +0.172077*god+0.059311*inherit+0.054774*jesus+0.051593*revelation+0.046836*son\n",
      "topic33  = +2.663946*art+0.038103*productions+0.038003*rap+0.037225*riddle+0.034499*ep\n",
      "topic34  = +0.129552*gordon+0.128938*surrender+0.127612*jxp+0.127584*shameful+0.127297*chastity\n",
      "topic35  = +0.101926*simms+0.094488*mb+0.064495*mhz+0.049046*vram+0.046815*ns\n",
      "topic36  = +0.617871*david+0.022742*koresh+0.022527*cents+0.022510*sternlight+0.021886*keyboard\n",
      "topic37  = +0.086784*key+0.052560*chip+0.042931*keys+0.037345*encryption+0.029920*clipper\n",
      "topic38  = +3.448788*ax+0.257231*max+0.107491*di+0.068102*bhj+0.066586*giz\n",
      "topic39  = +0.225020*card+0.123880*video+0.072716*drivers+0.063513*driver+0.040751*diamond\n",
      "topic40  = +0.308866*max+0.123560*ic+0.102766*hfe+0.095214*modem+0.087937*vce\n",
      "topic41  = +0.416623*monitor+0.254380*vga+0.127651*tracy+0.127492*mail+0.074369*mike\n",
      "topic42  = +4.780405*ax+0.029680*mg+0.029655*stage+0.028406*frame+0.028225*page\n",
      "topic43  = +0.253473*drive+0.144494*scsi+0.065513*talking+0.060823*ide+0.056480*drives\n",
      "topic44  = +0.359907*ticket+0.304285*duke+0.215150*gritz+0.137325*populist+0.111059*president\n",
      "topic45  = +0.629876*shipping+0.364494*included+0.146935*dennis+0.116840*manual+0.099869*box\n",
      "topic46  = +0.304081*anyone+0.249901*erickson+0.220414*scot+0.197299*scoop+0.092900*cci\n",
      "topic47  = +0.698802*offline+0.693928*contact+0.492849*rick+0.273006*wanted+0.017481*consortium\n",
      "topic48  = +0.711618*forsale+0.253210*misc+0.222789*audio+0.217087*na+0.197007*sony\n",
      "topic49  = +0.364655*cosmo+0.364655*proline+0.364655*alphalpha+0.359764*angmar+0.308585*bu\n",
      "( 0)                   alt.atheism topic00(7.6e-01) topic32(1.5e-01) topic16(3.2e-02)\n",
      "( 1)                 comp.graphics topic00(3.0e-01) topic12(2.7e-01) topic18(9.5e-02)\n",
      "( 2)       comp.os.ms-windows.misc topic00(2.3e-01) topic22(2.2e-01) topic12(2.1e-01)\n",
      "( 3)      comp.sys.ibm.pc.hardware topic00(2.5e-01) topic35(1.5e-01) topic43(1.5e-01)\n",
      "( 4)         comp.sys.mac.hardware topic00(3.4e-01) topic35(1.9e-01) topic43(8.0e-02)\n",
      "( 5)                comp.windows.x topic15(2.9e-01) topic00(2.4e-01) topic12(1.2e-01)\n",
      "( 6)                  misc.forsale topic08(3.0e-01) topic00(2.3e-01) topic07(5.8e-02)\n",
      "( 7)                     rec.autos topic00(8.5e-01) topic08(4.1e-02) topic18(3.7e-02)\n",
      "( 8)               rec.motorcycles topic00(8.5e-01) topic08(3.2e-02) topic18(2.8e-02)\n",
      "( 9)            rec.sport.baseball topic00(4.0e-01) topic07(4.0e-01) topic02(4.8e-02)\n",
      "(10)              rec.sport.hockey topic07(6.6e-01) topic00(2.8e-01) topic18(2.1e-02)\n",
      "(11)                     sci.crypt topic37(5.2e-01) topic00(4.0e-01) topic12(2.8e-02)\n",
      "(12)               sci.electronics topic00(5.4e-01) topic37(7.3e-02) topic18(6.8e-02)\n",
      "(13)                       sci.med topic00(6.7e-01) topic34(1.1e-01) topic18(3.4e-02)\n",
      "(14)                     sci.space topic00(7.5e-01) topic12(4.2e-02) topic18(3.1e-02)\n",
      "(15)        soc.religion.christian topic00(6.1e-01) topic32(2.7e-01) topic27(2.3e-02)\n",
      "(16)            talk.politics.guns topic00(8.6e-01) topic37(2.6e-02) topic07(1.5e-02)\n",
      "(17)         talk.politics.mideast topic00(6.2e-01) topic21(1.9e-01) topic49(2.2e-02)\n",
      "(18)            talk.politics.misc topic00(7.9e-01) topic37(2.6e-02) topic07(2.2e-02)\n",
      "(19)            talk.religion.misc topic00(6.8e-01) topic32(1.5e-01) topic16(2.6e-02)\n",
      "         topic0        talk.politics.guns(8.0e-02)                 rec.autos(8.0e-02)           rec.motorcycles(8.0e-02)\n",
      "         topic1  comp.sys.ibm.pc.hardware(8.2e-02)        talk.religion.misc(8.2e-02)        rec.sport.baseball(8.1e-02)\n",
      "         topic2        rec.sport.baseball(7.3e-02)               alt.atheism(7.3e-02)     talk.politics.mideast(6.2e-02)\n",
      "         topic3        rec.sport.baseball(1.1e-01)            comp.windows.x(1.1e-01)          rec.sport.hockey(7.7e-02)\n",
      "         topic4        rec.sport.baseball(1.6e-01)              misc.forsale(1.6e-01)        talk.politics.guns(6.4e-02)\n",
      "         topic5     comp.sys.mac.hardware(8.1e-02)        talk.politics.misc(8.1e-02)           sci.electronics(6.1e-02)\n",
      "         topic6                 rec.autos(1.0e-01)            comp.windows.x(1.0e-01)  comp.sys.ibm.pc.hardware(8.9e-02)\n",
      "         topic7          rec.sport.hockey(2.9e-01)        rec.sport.baseball(2.9e-01)              misc.forsale(4.1e-02)\n",
      "         topic8              misc.forsale(7.3e-02)                 rec.autos(7.3e-02)           rec.motorcycles(5.6e-02)\n",
      "         topic9          rec.sport.hockey(6.8e-02)           sci.electronics(6.8e-02)              misc.forsale(5.6e-02)\n",
      "        topic10  comp.sys.ibm.pc.hardware(1.4e-01)     comp.sys.mac.hardware(1.4e-01)              misc.forsale(1.1e-01)\n",
      "        topic11            comp.windows.x(1.1e-01)             comp.graphics(1.1e-01)        rec.sport.baseball(7.7e-02)\n",
      "        topic12             comp.graphics(2.4e-01)   comp.os.ms-windows.misc(2.4e-01)            comp.windows.x(1.3e-01)\n",
      "        topic13             comp.graphics(9.5e-02)              misc.forsale(9.5e-02)            comp.windows.x(7.1e-02)\n",
      "        topic14              misc.forsale(1.0e-01)             comp.graphics(1.0e-01)  comp.sys.ibm.pc.hardware(9.5e-02)\n",
      "        topic15            comp.windows.x(1.2e-01)   comp.os.ms-windows.misc(1.2e-01)             comp.graphics(6.6e-02)\n",
      "        topic16               alt.atheism(1.3e-01)        talk.religion.misc(1.3e-01)   comp.os.ms-windows.misc(9.4e-02)\n",
      "        topic17        rec.sport.baseball(6.9e-02)     comp.sys.mac.hardware(6.9e-02)          rec.sport.hockey(4.5e-02)\n",
      "        topic18             comp.graphics(1.1e-01)            comp.windows.x(1.1e-01)  comp.sys.ibm.pc.hardware(1.0e-01)\n",
      "        topic19   comp.os.ms-windows.misc(3.2e-03)              misc.forsale(3.2e-03)  comp.sys.ibm.pc.hardware(2.5e-03)\n",
      "        topic20                   sci.med(1.0e-01)           rec.motorcycles(1.0e-01)              misc.forsale(8.1e-02)\n",
      "        topic21     talk.politics.mideast(4.4e-02)        talk.religion.misc(4.4e-02)        talk.politics.misc(4.1e-02)\n",
      "        topic22   comp.os.ms-windows.misc(1.4e-01)  comp.sys.ibm.pc.hardware(1.4e-01)            comp.windows.x(1.0e-01)\n",
      "        topic23    soc.religion.christian(2.7e-01)               alt.atheism(2.7e-01)        talk.religion.misc(7.8e-02)\n",
      "        topic24     talk.politics.mideast(9.0e-02)          rec.sport.hockey(9.0e-02)                   sci.med(6.7e-02)\n",
      "        topic25        talk.religion.misc(1.2e-01)            comp.windows.x(1.2e-01)             comp.graphics(9.5e-02)\n",
      "        topic26        talk.politics.misc(2.9e-02)        talk.religion.misc(2.9e-02)                 sci.space(1.7e-02)\n",
      "        topic27           sci.electronics(1.1e-01)    soc.religion.christian(1.1e-01)   comp.os.ms-windows.misc(8.6e-02)\n",
      "        topic28  comp.sys.ibm.pc.hardware(1.6e-01)     comp.sys.mac.hardware(1.6e-01)   comp.os.ms-windows.misc(1.5e-01)\n",
      "        topic29   comp.os.ms-windows.misc(1.5e-01)     talk.politics.mideast(1.5e-01)    soc.religion.christian(6.9e-02)\n",
      "        topic30                 sci.crypt(6.1e-02)        talk.politics.guns(6.1e-02)           rec.motorcycles(5.1e-02)\n",
      "        topic31     comp.sys.mac.hardware(1.4e-01)           rec.motorcycles(1.4e-01)              misc.forsale(8.3e-02)\n",
      "        topic32    soc.religion.christian(2.4e-01)        talk.religion.misc(2.4e-01)               alt.atheism(2.3e-01)\n",
      "        topic33     comp.sys.mac.hardware(1.5e-01)        rec.sport.baseball(1.5e-01)              misc.forsale(8.3e-02)\n",
      "        topic34                   sci.med(3.2e-02)        talk.religion.misc(3.2e-02)  comp.sys.ibm.pc.hardware(2.4e-02)\n",
      "        topic35     comp.sys.mac.hardware(2.8e-01)  comp.sys.ibm.pc.hardware(2.8e-01)              misc.forsale(7.8e-02)\n",
      "        topic36        rec.sport.baseball(9.7e-02)        talk.religion.misc(9.7e-02)          rec.sport.hockey(8.5e-02)\n",
      "        topic37                 sci.crypt(7.1e-02)           sci.electronics(7.1e-02)            comp.windows.x(6.6e-02)\n",
      "        topic38   comp.os.ms-windows.misc(1.6e-02)                 sci.crypt(1.6e-02)             comp.graphics(5.7e-03)\n",
      "        topic39  comp.sys.ibm.pc.hardware(2.0e-01)   comp.os.ms-windows.misc(2.0e-01)             comp.graphics(1.5e-01)\n",
      "        topic40           sci.electronics(1.7e-01)           rec.motorcycles(1.7e-01)     comp.sys.mac.hardware(1.1e-01)\n",
      "        topic41     comp.sys.mac.hardware(1.9e-01)              misc.forsale(1.9e-01)  comp.sys.ibm.pc.hardware(1.7e-01)\n",
      "        topic42   comp.os.ms-windows.misc(4.1e-02)              misc.forsale(4.1e-02)  comp.sys.ibm.pc.hardware(2.8e-02)\n",
      "        topic43  comp.sys.ibm.pc.hardware(2.0e-01)     comp.sys.mac.hardware(2.0e-01)              misc.forsale(1.4e-01)\n",
      "        topic44        talk.politics.guns(1.2e-01)              misc.forsale(1.2e-01)        talk.politics.misc(1.2e-01)\n",
      "        topic45              misc.forsale(6.2e-02)     comp.sys.mac.hardware(6.2e-02)           sci.electronics(4.7e-02)\n",
      "        topic46        rec.sport.baseball(1.0e-01)           sci.electronics(1.0e-01)     comp.sys.mac.hardware(8.3e-02)\n",
      "        topic47              misc.forsale(1.9e-01)             comp.graphics(1.9e-01)          rec.sport.hockey(8.6e-02)\n",
      "        topic48              misc.forsale(9.0e-02)  comp.sys.ibm.pc.hardware(9.0e-02)           sci.electronics(8.7e-02)\n",
      "        topic49     talk.politics.mideast(7.1e-02)        talk.politics.misc(7.1e-02)   comp.os.ms-windows.misc(5.9e-02)\n",
      "should be close to one: [1.4750062  0.81859083 1.03528953 ... 1.02146015 1.30758872 1.13267334]\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.display(df.head(n=5))\n",
    "nmf_lsa.print_mat(max_docs=5, max_topics=100, max_terms=5)\n",
    "print_lsa(nmf_lsa)\n",
    "print(\"should be close to one:\", nmf_lsa.topic_doc_mat.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want 50 topics\n",
      "[    0] dH 173731.00311718334\n",
      "[    0] dW 0.13011962473768077\n",
      "[   10] dH 43.75320931600517\n",
      "[   10] dW 0.0021964432192927\n",
      "[   20] dH 10.16039695897526\n",
      "[   20] dW 0.0002167466490720534\n",
      "[   30] dH 3.744576559969413\n",
      "[   30] dW 7.880204684271022e-05\n",
      "[   40] dH 1.8026573102677164\n",
      "[   40] dW 7.4008139353296e-05\n",
      "[   50] dH 0.9815556203840963\n",
      "[   50] dW 3.181144416666826e-05\n",
      "[   60] dH 0.643131947909195\n",
      "[   60] dW 8.160193563109852e-06\n",
      "[   70] dH 0.4356999012240623\n",
      "[   70] dW 5.438912617495792e-06\n",
      "[   80] dH 0.3274124685320499\n",
      "[   80] dW 3.334353922541182e-06\n",
      "[   90] dH 0.28487979204805475\n",
      "[   90] dW 2.3489638388330787e-06\n",
      "[  100] dH 0.22366845884589548\n",
      "[  100] dW 2.3851281172306353e-06\n",
      "[  110] dH 0.19107847119341528\n",
      "[  110] dW 1.8914145464074424e-06\n",
      "[  120] dH 0.18863249233578083\n",
      "[  120] dW 1.6858389152326332e-06\n",
      "[  130] dH 0.14554042030697056\n",
      "[  130] dW 1.2898878085676262e-06\n",
      "[  140] dH 0.18710525757715668\n",
      "[  140] dW 1.3842593849492278e-06\n",
      "[  150] dH 0.13534173982820527\n",
      "[  150] dW 9.786599795918668e-07\n",
      "[  160] dH 0.07968305391914995\n",
      "[  160] dW 6.902478693394941e-07\n",
      "[  170] dH 0.07350920348499537\n",
      "[  170] dW 7.641897384718028e-07\n",
      "[  180] dH 0.062085347341062705\n",
      "[  180] dW 8.590202490304744e-07\n",
      "[  190] dH 0.06906066123591391\n",
      "[  190] dW 9.54366644544508e-07\n"
     ]
    }
   ],
   "source": [
    "#N_topics = 3+0*min(3, n_catogories)\n",
    "print(\"We want %d topics\"%N_topics)\n",
    "\n",
    "plsa=pLSA()\n",
    "plsa.set_structured_documents(email_sdocs)\n",
    "plsa.topic_decompose(ntopics=N_topics, on_update=on_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc0  = +4.6e-01*topic0+2.9e-01*topic13+1.8e-01*topic27+3.8e-02*topic11+2.9e-02*topic5+6.4e-04*topic46+2.0e-05*topic20+1.3e-08*topic12+5.9e-09*topic8+1.2e-09*topic3+1.1e-09*topic42+6.0e-10*topic38+4.2e-10*topic24+4.1e-10*topic43+2.8e-10*topic49+2.7e-10*topic7+1.6e-10*topic2+1.4e-10*topic48+1.3e-10*topic33+1.1e-10*topic39+1.1e-10*topic6+1.0e-10*topic26+8.8e-11*topic17+6.5e-11*topic41+4.7e-11*topic14+4.1e-11*topic36+3.9e-11*topic35+3.5e-11*topic29+3.3e-11*topic23+1.8e-11*topic22+1.6e-11*topic4+1.0e-11*topic1+7.0e-12*topic47+4.3e-12*topic15+2.5e-12*topic44+2.3e-12*topic18+1.4e-12*topic32+1.1e-12*topic25+2.8e-13*topic34+2.2e-13*topic10+1.3e-13*topic21+4.6e-14*topic40+1.9e-14*topic45+1.5e-15*topic30+1.0e-18*topic19+3.7e-22*topic16+3.5e-24*topic9+1.9e-24*topic28+1.1e-24*topic31+1.4e-36*topic37\n",
      "   doc1  = +3.1e-01*topic47+2.8e-01*topic8+1.3e-01*topic29+1.3e-01*topic13+1.3e-01*topic9+1.6e-02*topic33+7.1e-03*topic36+5.4e-06*topic40+2.6e-06*topic4+5.7e-08*topic26+8.0e-09*topic25+5.0e-09*topic15+2.3e-09*topic3+1.9e-09*topic5+9.6e-10*topic27+6.6e-10*topic30+4.4e-10*topic6+2.0e-10*topic22+1.3e-10*topic39+1.0e-10*topic1+7.7e-11*topic45+5.2e-11*topic28+4.9e-11*topic0+4.8e-11*topic17+4.6e-11*topic21+2.0e-11*topic38+1.9e-11*topic24+1.7e-11*topic46+1.4e-11*topic14+3.9e-12*topic43+1.9e-12*topic49+1.6e-12*topic41+1.5e-12*topic19+1.3e-12*topic35+2.8e-13*topic42+1.1e-14*topic11+6.6e-15*topic44+5.2e-15*topic18+5.1e-15*topic12+2.7e-15*topic48+1.5e-15*topic23+3.4e-17*topic10+5.9e-20*topic7+1.4e-22*topic16+3.8e-25*topic31+6.9e-26*topic32+4.5e-36*topic20+4.7e-40*topic34+1.1e-41*topic2+5.9e-45*topic37\n",
      "   doc2  = +6.0e-01*topic0+9.7e-02*topic8+9.6e-02*topic13+5.8e-02*topic25+5.2e-02*topic26+4.7e-02*topic5+4.2e-02*topic47+6.6e-03*topic42+5.1e-05*topic32+4.3e-09*topic1+3.0e-09*topic30+2.4e-09*topic29+1.1e-09*topic35+1.1e-09*topic15+8.0e-10*topic6+4.7e-10*topic23+3.6e-10*topic16+2.2e-10*topic39+1.5e-10*topic48+1.4e-10*topic38+1.2e-10*topic11+1.1e-10*topic28+6.2e-11*topic17+4.7e-11*topic21+4.6e-11*topic36+3.5e-11*topic14+3.5e-11*topic22+1.6e-11*topic34+1.5e-11*topic43+1.0e-11*topic46+8.1e-12*topic4+6.9e-12*topic41+5.8e-12*topic45+4.5e-12*topic49+3.7e-12*topic44+1.6e-12*topic24+1.0e-12*topic7+5.3e-13*topic9+4.9e-14*topic33+7.9e-15*topic31+4.5e-16*topic3+5.5e-17*topic27+1.0e-18*topic12+4.2e-19*topic10+2.0e-24*topic18+4.6e-29*topic19+1.6e-29*topic37+4.8e-30*topic40+2.4e-39*topic20+6.4e-47*topic2\n",
      "   doc3  = +5.2e-01*topic33+3.8e-01*topic2+5.2e-02*topic25+4.6e-02*topic26+1.4e-09*topic7+1.2e-09*topic46+6.9e-10*topic40+5.2e-10*topic11+4.6e-10*topic1+4.5e-10*topic43+3.7e-10*topic32+3.4e-10*topic21+3.4e-10*topic0+3.3e-10*topic6+3.2e-10*topic14+3.1e-10*topic29+2.8e-10*topic3+2.6e-10*topic38+2.6e-10*topic35+2.3e-10*topic36+2.2e-10*topic42+2.1e-10*topic39+2.0e-10*topic12+1.5e-10*topic34+1.5e-10*topic30+1.4e-10*topic18+1.3e-10*topic24+1.2e-10*topic17+1.2e-10*topic28+1.1e-10*topic22+1.1e-10*topic15+9.6e-11*topic10+9.6e-11*topic41+8.2e-11*topic37+6.2e-11*topic47+3.4e-11*topic8+2.3e-11*topic23+1.7e-11*topic16+8.2e-12*topic48+3.5e-12*topic44+1.6e-12*topic45+3.9e-13*topic27+3.0e-13*topic31+2.9e-13*topic49+1.7e-16*topic9+2.9e-22*topic4+2.2e-23*topic19+5.7e-25*topic13+5.7e-55*topic5+2.2e-110*topic20\n",
      "   doc4  = +5.1e-01*topic2+2.2e-01*topic13+2.0e-01*topic4+6.2e-02*topic20+4.9e-09*topic23+2.5e-09*topic12+9.4e-10*topic43+8.1e-10*topic47+7.5e-10*topic38+6.8e-10*topic40+6.3e-10*topic0+6.1e-10*topic45+6.0e-10*topic32+5.1e-10*topic35+4.8e-10*topic14+4.6e-10*topic3+4.5e-10*topic41+4.4e-10*topic11+4.1e-10*topic5+4.1e-10*topic18+3.9e-10*topic46+3.5e-10*topic30+3.3e-10*topic36+3.3e-10*topic9+2.9e-10*topic39+2.7e-10*topic17+2.7e-10*topic42+2.6e-10*topic27+2.5e-10*topic6+2.3e-10*topic24+2.1e-10*topic15+2.0e-10*topic22+1.7e-10*topic1+1.7e-10*topic28+1.7e-10*topic34+1.6e-10*topic21+1.2e-10*topic29+1.1e-10*topic48+8.9e-11*topic10+8.0e-11*topic8+5.0e-11*topic25+3.3e-11*topic26+7.6e-12*topic44+3.0e-12*topic37+2.8e-13*topic49+2.7e-13*topic31+2.2e-13*topic16+1.5e-13*topic7+3.9e-14*topic33+7.7e-22*topic19\n",
      "   doc5  = +8.7e-01*topic0+6.2e-02*topic24+4.9e-02*topic23+1.5e-02*topic13+2.5e-07*topic20+1.3e-07*topic5+1.7e-08*topic32+1.3e-09*topic18+1.1e-09*topic10+9.3e-10*topic29+7.4e-10*topic27+4.3e-10*topic30+2.7e-10*topic42+2.1e-10*topic21+2.0e-10*topic39+1.8e-10*topic3+1.5e-10*topic16+1.4e-10*topic36+1.3e-10*topic38+1.3e-10*topic34+1.1e-10*topic25+8.7e-11*topic1+8.2e-11*topic49+8.0e-11*topic8+6.7e-11*topic26+5.2e-11*topic17+4.6e-11*topic2+3.5e-11*topic14+3.5e-11*topic22+3.5e-11*topic4+3.1e-11*topic35+2.8e-11*topic46+2.8e-11*topic15+2.7e-11*topic6+2.4e-11*topic28+1.3e-11*topic43+9.6e-12*topic33+8.3e-12*topic47+6.2e-12*topic31+2.3e-12*topic48+1.5e-12*topic40+5.9e-13*topic41+3.9e-13*topic12+3.9e-13*topic44+3.7e-13*topic19+1.3e-13*topic11+3.4e-14*topic37+7.2e-15*topic7+8.7e-19*topic45+5.7e-22*topic9\n",
      "   doc6  = +7.4e-01*topic25+1.5e-01*topic13+8.9e-02*topic29+2.7e-02*topic24+6.3e-08*topic0+1.0e-08*topic1+8.4e-09*topic15+7.6e-09*topic6+6.8e-09*topic9+4.7e-09*topic4+3.8e-09*topic43+3.7e-09*topic37+3.0e-09*topic40+3.0e-09*topic42+2.6e-09*topic49+2.4e-09*topic26+2.3e-09*topic32+2.0e-09*topic11+1.9e-09*topic36+1.9e-09*topic34+1.8e-09*topic38+1.5e-09*topic14+1.4e-09*topic46+1.3e-09*topic39+1.1e-09*topic3+1.1e-09*topic21+8.3e-10*topic35+4.8e-10*topic17+4.1e-10*topic47+3.2e-10*topic22+3.2e-10*topic12+2.9e-10*topic48+2.5e-10*topic33+1.9e-10*topic28+1.4e-10*topic41+1.1e-10*topic23+6.6e-11*topic30+3.8e-11*topic7+3.2e-11*topic44+4.6e-12*topic8+1.4e-12*topic10+4.9e-14*topic31+1.3e-14*topic18+3.3e-15*topic2+4.1e-20*topic16+3.4e-22*topic27+2.2e-22*topic45+1.5e-38*topic19+4.3e-40*topic5+2.4e-76*topic20\n",
      "   doc7  = +3.8e-01*topic25+2.1e-01*topic2+1.4e-01*topic13+1.2e-01*topic20+7.9e-02*topic23+4.6e-02*topic5+1.6e-02*topic45+1.4e-06*topic7+8.3e-09*topic32+2.9e-09*topic36+2.2e-10*topic18+2.2e-10*topic24+1.5e-10*topic33+1.4e-10*topic29+1.2e-10*topic3+9.5e-11*topic48+8.6e-11*topic11+7.9e-11*topic39+6.6e-11*topic38+5.6e-11*topic31+4.6e-11*topic35+3.7e-11*topic30+3.7e-11*topic16+3.1e-11*topic46+9.1e-12*topic17+5.8e-12*topic6+2.8e-12*topic34+1.7e-12*topic26+1.3e-12*topic44+9.2e-13*topic14+5.6e-13*topic49+5.4e-13*topic22+3.1e-13*topic1+2.6e-13*topic41+1.6e-13*topic4+8.8e-14*topic9+3.4e-14*topic0+1.0e-14*topic40+4.0e-15*topic27+2.6e-15*topic28+6.4e-16*topic10+8.4e-17*topic15+3.2e-17*topic19+2.3e-18*topic42+4.9e-21*topic43+1.6e-21*topic8+8.1e-23*topic12+2.7e-24*topic37+1.5e-27*topic21+1.8e-33*topic47\n",
      "   doc8  = +6.6e-01*topic18+1.9e-01*topic10+8.4e-02*topic37+3.3e-02*topic24+3.1e-02*topic29+9.2e-09*topic1+7.0e-09*topic4+2.9e-09*topic11+1.8e-09*topic32+1.4e-09*topic16+1.1e-09*topic28+1.1e-09*topic22+7.4e-10*topic43+7.0e-10*topic30+6.7e-10*topic46+4.4e-10*topic0+3.8e-10*topic38+3.0e-10*topic36+2.8e-10*topic31+2.7e-10*topic14+2.2e-10*topic35+2.0e-10*topic40+1.9e-10*topic42+1.8e-10*topic39+1.8e-10*topic26+1.1e-10*topic15+1.0e-10*topic2+7.7e-11*topic48+6.9e-11*topic21+6.5e-11*topic6+4.9e-11*topic41+4.6e-11*topic34+3.7e-11*topic33+2.9e-11*topic49+2.6e-11*topic47+2.0e-11*topic17+1.3e-11*topic23+1.2e-11*topic3+8.2e-12*topic27+4.9e-12*topic25+4.6e-12*topic44+3.5e-12*topic45+1.6e-12*topic8+2.0e-15*topic5+2.1e-19*topic12+2.2e-20*topic9+3.7e-21*topic7+2.2e-29*topic19+1.2e-35*topic20+4.4e-44*topic13\n",
      "   doc9  = +5.0e-01*topic16+1.8e-01*topic20+1.6e-01*topic18+7.7e-02*topic30+6.5e-02*topic22+1.7e-02*topic45+6.4e-06*topic3+3.4e-09*topic24+1.5e-09*topic17+5.3e-10*topic8+5.2e-10*topic11+3.7e-10*topic29+2.4e-10*topic10+2.1e-10*topic26+2.0e-10*topic38+1.7e-10*topic4+1.7e-10*topic28+1.3e-10*topic0+9.2e-11*topic15+8.3e-11*topic43+8.1e-11*topic23+8.1e-11*topic39+7.1e-11*topic14+6.2e-11*topic36+5.5e-11*topic32+4.5e-11*topic35+4.0e-11*topic33+3.9e-11*topic1+3.4e-11*topic2+3.4e-11*topic6+2.5e-11*topic42+2.4e-11*topic47+1.5e-11*topic49+1.4e-11*topic34+6.9e-12*topic25+6.6e-12*topic41+3.6e-12*topic46+2.5e-12*topic48+2.5e-12*topic44+2.3e-12*topic31+6.3e-13*topic5+4.8e-15*topic40+1.5e-19*topic19+2.5e-24*topic12+8.9e-27*topic37+1.8e-27*topic7+6.1e-28*topic13+1.8e-29*topic21+3.2e-32*topic27+4.3e-35*topic9\n",
      "\n",
      " topic0  = +0.022142*car+0.010924*bike+0.010781*dod\n",
      " topic1  = +0.017622*writes+0.013950*article+0.013300*san\n",
      " topic2  = +0.042716*windows+0.024689*dos+0.011690*file\n",
      " topic3  = +0.012498*islam+0.010814*one+0.010028*writes\n",
      " topic4  = +0.032990*window+0.013749*display+0.012039*screen\n",
      " topic5  = +0.018098*time+0.015573*problem+0.010883*years\n",
      " topic6  = +0.018410*hockey+0.016378*team+0.010487*nhl\n",
      " topic7  = +0.014943*list+0.010132*information+0.010015*university\n",
      " topic8  = +0.020163*writes+0.013945*article+0.009217*like\n",
      " topic9  = +0.014220*posting+0.011063*group+0.010607*internet\n",
      "topic10  = +0.020455*law+0.018634*state+0.010532*court\n",
      "topic11  = +0.019245*turkish+0.015797*armenian+0.012351*armenians\n",
      "topic12  = +0.009190*south+0.008373*launch+0.007105*war\n",
      "topic13  = +0.045078*thanks+0.036044*please+0.035338*anyone\n",
      "topic14  = +0.034828*space+0.012879*nasa+0.009117*earth\n",
      "topic15  = +0.013433*ground+0.010491*wire+0.009797*use\n",
      "topic16  = +0.018406*god+0.010317*believe+0.009829*one\n",
      "topic17  = +0.022063*jews+0.020537*israel+0.016752*israeli\n",
      "topic18  = +0.012074*writes+0.011839*moral+0.009877*system\n",
      "topic19  = +0.863285*ax+0.063455*max+0.004619*pl\n",
      "topic20  = +0.022222*like+0.021777*think+0.020026*writes\n",
      "topic21  = +0.023077*gun+0.014051*file+0.009648*firearms\n",
      "topic22  = +0.036649*god+0.017757*jesus+0.011277*church\n",
      "topic23  = +0.010197*writes+0.010141*article+0.006565*like\n",
      "topic24  = +0.053938*writes+0.051810*article+0.012817*david\n",
      "topic25  = +0.024488*db+0.018951*price+0.014316*sale\n",
      "topic26  = +0.013642*widget+0.011848*application+0.008583*app\n",
      "topic27  = +0.015603*said+0.012673*one+0.011496*went\n",
      "topic28  = +0.016042*israel+0.008641*war+0.008390*israeli\n",
      "topic29  = +0.037381*drive+0.014502*disk+0.012468*hard\n",
      "topic30  = +0.018389*people+0.010162*objective+0.007939*article\n",
      "topic31  = +0.014611*money+0.013846*government+0.008199*people\n",
      "topic32  = +0.009444*one+0.007745*marriage+0.007006*ra\n",
      "topic33  = +0.010854*image+0.010532*available+0.008538*edu\n",
      "topic34  = +0.018755*encryption+0.014891*key+0.010738*government\n",
      "topic35  = +0.015229*game+0.011849*play+0.008782*team\n",
      "topic36  = +0.011744*health+0.010786*keyboard+0.009335*medical\n",
      "topic37  = +0.011884*gun+0.010940*guns+0.010295*people\n",
      "topic38  = +0.010428*pa+0.010320*um+0.008839*ei\n",
      "topic39  = +0.036718*cx+0.022300*ww+0.020410*hz\n",
      "topic40  = +0.030808*water+0.010470*good+0.009567*dept\n",
      "topic41  = +0.010907*writes+0.010770*fbi+0.010053*article\n",
      "topic42  = +0.015582*year+0.009607*game+0.008716*team\n",
      "topic43  = +0.007028*disease+0.005944*one+0.005768*people\n",
      "topic44  = +0.499730*ax+0.030968*max+0.024105*di\n",
      "topic45  = +0.030264*mr+0.023241*president+0.013384*stephanopoulos\n",
      "topic46  = +0.010116*one+0.010079*jesus+0.006898*matthew\n",
      "topic47  = +0.020783*scsi+0.018736*card+0.017061*mb\n",
      "topic48  = +0.027341*file+0.023125*output+0.022382*entry\n",
      "topic49  = +0.035866*key+0.015895*chip+0.012746*keys\n",
      "( 0)                   alt.atheism topic18(2.2e-01) topic16(1.7e-01) topic03(1.2e-01)\n",
      "( 1)                 comp.graphics topic33(2.2e-01) topic13(1.8e-01) topic04(8.5e-02)\n",
      "( 2)       comp.os.ms-windows.misc topic02(4.1e-01) topic13(1.2e-01) topic47(5.8e-02)\n",
      "( 3)      comp.sys.ibm.pc.hardware topic47(2.6e-01) topic13(1.4e-01) topic29(1.4e-01)\n",
      "( 4)         comp.sys.mac.hardware topic47(2.6e-01) topic13(1.2e-01) topic05(1.0e-01)\n",
      "( 5)                comp.windows.x topic04(3.1e-01) topic33(1.6e-01) topic13(1.2e-01)\n",
      "( 6)                  misc.forsale topic25(3.9e-01) topic13(7.6e-02) topic47(5.3e-02)\n",
      "( 7)                     rec.autos topic00(3.3e-01) topic05(1.1e-01) topic20(9.3e-02)\n",
      "( 8)               rec.motorcycles topic00(4.4e-01) topic20(1.0e-01) topic05(6.0e-02)\n",
      "( 9)            rec.sport.baseball topic42(4.8e-01) topic08(1.2e-01) topic20(9.7e-02)\n",
      "(10)              rec.sport.hockey topic35(3.5e-01) topic06(2.4e-01) topic20(1.5e-01)\n",
      "(11)                     sci.crypt topic49(3.1e-01) topic34(2.2e-01) topic20(1.1e-01)\n",
      "(12)               sci.electronics topic29(1.6e-01) topic15(1.5e-01) topic13(1.3e-01)\n",
      "(13)                       sci.med topic43(3.2e-01) topic24(8.5e-02) topic20(6.5e-02)\n",
      "(14)                     sci.space topic14(3.1e-01) topic20(1.1e-01) topic31(7.4e-02)\n",
      "(15)        soc.religion.christian topic22(3.4e-01) topic16(1.4e-01) topic46(8.5e-02)\n",
      "(16)            talk.politics.guns topic41(2.1e-01) topic37(1.5e-01) topic21(1.5e-01)\n",
      "(17)         talk.politics.mideast topic17(2.5e-01) topic11(1.9e-01) topic28(1.3e-01)\n",
      "(18)            talk.politics.misc topic31(1.5e-01) topic01(1.1e-01) topic37(9.6e-02)\n",
      "(19)            talk.religion.misc topic22(1.6e-01) topic41(9.9e-02) topic46(9.6e-02)\n",
      "         topic0           rec.motorcycles(3.3e-01)                 rec.autos(3.3e-01)              misc.forsale(2.8e-02)\n",
      "         topic1        talk.politics.misc(4.3e-02)        rec.sport.baseball(4.3e-02)           rec.motorcycles(4.0e-02)\n",
      "         topic2   comp.os.ms-windows.misc(1.1e-01)  comp.sys.ibm.pc.hardware(1.1e-01)     comp.sys.mac.hardware(5.1e-02)\n",
      "         topic3               alt.atheism(2.2e-02)    soc.religion.christian(2.2e-02)        talk.religion.misc(1.8e-02)\n",
      "         topic4            comp.windows.x(8.5e-02)             comp.graphics(8.5e-02)   comp.os.ms-windows.misc(3.1e-02)\n",
      "         topic5                 rec.autos(1.0e-01)     comp.sys.mac.hardware(1.0e-01)  comp.sys.ibm.pc.hardware(6.6e-02)\n",
      "         topic6          rec.sport.hockey(1.6e-02)        rec.sport.baseball(1.6e-02)              misc.forsale(9.1e-03)\n",
      "         topic7             comp.graphics(4.0e-02)              misc.forsale(4.0e-02)            comp.windows.x(3.7e-02)\n",
      "         topic8        rec.sport.baseball(3.4e-02)          rec.sport.hockey(3.4e-02)           sci.electronics(3.2e-02)\n",
      "         topic9     talk.politics.mideast(2.5e-02)                 sci.crypt(2.5e-02)              misc.forsale(2.1e-02)\n",
      "        topic10        talk.politics.misc(3.5e-02)     talk.politics.mideast(3.5e-02)        talk.politics.guns(2.5e-02)\n",
      "        topic11     talk.politics.mideast(4.0e-03)        talk.politics.misc(4.0e-03)    soc.religion.christian(3.6e-03)\n",
      "        topic12                 sci.space(3.7e-02)        talk.politics.misc(3.7e-02)     talk.politics.mideast(2.3e-02)\n",
      "        topic13             comp.graphics(1.4e-01)  comp.sys.ibm.pc.hardware(1.4e-01)           sci.electronics(1.3e-01)\n",
      "        topic14                 sci.space(1.4e-02)           sci.electronics(1.4e-02)             comp.graphics(1.1e-02)\n",
      "        topic15           sci.electronics(2.6e-02)                 rec.autos(2.6e-02)           rec.motorcycles(2.1e-02)\n",
      "        topic16               alt.atheism(1.4e-01)    soc.religion.christian(1.4e-01)        talk.religion.misc(7.1e-02)\n",
      "        topic17     talk.politics.mideast(5.1e-02)                   sci.med(5.1e-02)        talk.politics.guns(9.3e-03)\n",
      "        topic18               alt.atheism(5.0e-02)        talk.religion.misc(5.0e-02)    soc.religion.christian(4.3e-02)\n",
      "        topic19   comp.os.ms-windows.misc(1.0e-03)           rec.motorcycles(1.0e-03)           sci.electronics(6.5e-04)\n",
      "        topic20          rec.sport.hockey(1.1e-01)                 sci.crypt(1.1e-01)                 sci.space(1.1e-01)\n",
      "        topic21        talk.politics.guns(1.7e-02)                 rec.autos(1.7e-02)           rec.motorcycles(9.0e-03)\n",
      "        topic22    soc.religion.christian(1.6e-01)        talk.religion.misc(1.6e-01)               alt.atheism(3.8e-02)\n",
      "        topic23        talk.politics.guns(6.5e-02)                 sci.space(6.5e-02)        talk.politics.misc(4.3e-02)\n",
      "        topic24                   sci.med(6.3e-02)                 rec.autos(6.3e-02)           rec.motorcycles(4.5e-02)\n",
      "        topic25              misc.forsale(7.0e-02)     comp.sys.mac.hardware(7.0e-02)           sci.electronics(5.2e-02)\n",
      "        topic26            comp.windows.x(2.6e-02)              misc.forsale(2.6e-02)     comp.sys.mac.hardware(1.5e-02)\n",
      "        topic27     talk.politics.mideast(2.8e-02)                 rec.autos(2.8e-02)           rec.motorcycles(2.3e-02)\n",
      "        topic28     talk.politics.mideast(2.8e-02)        talk.religion.misc(2.8e-02)        talk.politics.misc(2.3e-02)\n",
      "        topic29           sci.electronics(1.4e-01)  comp.sys.ibm.pc.hardware(1.4e-01)     comp.sys.mac.hardware(8.2e-02)\n",
      "        topic30        talk.religion.misc(5.6e-02)               alt.atheism(5.6e-02)     talk.politics.mideast(3.5e-02)\n",
      "        topic31        talk.politics.misc(7.4e-02)                 sci.space(7.4e-02)        talk.politics.guns(2.2e-02)\n",
      "        topic32        talk.religion.misc(3.3e-02)               alt.atheism(3.3e-02)    soc.religion.christian(3.3e-02)\n",
      "        topic33             comp.graphics(1.6e-01)            comp.windows.x(1.6e-01)   comp.os.ms-windows.misc(3.1e-02)\n",
      "        topic34                 sci.crypt(4.6e-03)        talk.politics.misc(4.6e-03)           sci.electronics(4.0e-03)\n",
      "        topic35          rec.sport.hockey(9.0e-03)        rec.sport.baseball(9.0e-03)              misc.forsale(5.4e-03)\n",
      "        topic36                   sci.med(9.0e-03)        talk.politics.misc(9.0e-03)     comp.sys.mac.hardware(7.3e-03)\n",
      "        topic37        talk.politics.guns(9.6e-02)        talk.politics.misc(9.6e-02)                 sci.crypt(3.0e-02)\n",
      "        topic38           sci.electronics(1.4e-02)   comp.os.ms-windows.misc(1.4e-02)              misc.forsale(5.7e-03)\n",
      "        topic39   comp.os.ms-windows.misc(2.4e-03)              misc.forsale(2.4e-03)  comp.sys.ibm.pc.hardware(2.0e-03)\n",
      "        topic40              misc.forsale(2.6e-02)                   sci.med(2.6e-02)           sci.electronics(1.9e-02)\n",
      "        topic41        talk.politics.guns(9.9e-02)        talk.religion.misc(9.9e-02)        talk.politics.misc(8.1e-02)\n",
      "        topic42        rec.sport.baseball(1.3e-02)          rec.sport.hockey(1.3e-02)                 rec.autos(9.9e-03)\n",
      "        topic43                   sci.med(1.5e-02)    soc.religion.christian(1.5e-02)                 sci.space(9.0e-03)\n",
      "        topic44   comp.os.ms-windows.misc(9.6e-04)            comp.windows.x(9.6e-04)              misc.forsale(8.0e-04)\n",
      "        topic45        talk.politics.misc(3.6e-02)        talk.politics.guns(3.6e-02)                 sci.crypt(2.3e-02)\n",
      "        topic46        talk.religion.misc(8.5e-02)    soc.religion.christian(8.5e-02)               alt.atheism(8.4e-02)\n",
      "        topic47  comp.sys.ibm.pc.hardware(2.6e-01)     comp.sys.mac.hardware(2.6e-01)             comp.graphics(6.7e-02)\n",
      "        topic48            comp.windows.x(1.9e-02)   comp.os.ms-windows.misc(1.9e-02)             comp.graphics(1.3e-02)\n",
      "        topic49                 sci.crypt(1.8e-02)           sci.electronics(1.8e-02)            comp.windows.x(7.2e-03)\n"
     ]
    }
   ],
   "source": [
    "plsa.print_mat(max_docs=10, max_terms=3, max_topics=100)\n",
    "print_lsa(plsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    N_topics = 3+0*min(3, n_catogories)\n",
    "    print(\"We want %d topics\"%N_topics)\n",
    "    lda=LDA()\n",
    "    lda.set_structured_documents(email_sdocs)\n",
    "    lda.topic_decompose(ntopics=N_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    lda.print_mat(max_docs=10, max_terms=3)\n",
    "    print_lsa(lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bakcups\n",
    "## 随机梯度下降 (Stochastic gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to maximize\n",
    "$$\n",
    "P(\\theta, \\phi|\\mathbf{w}) = \\sum_{z}P(\\theta, \\phi|\\mathbf{z}, \\mathbf{w}) P(\\mathbf{z}|\\mathbf{w})\n",
    "$$\n",
    "\n",
    "then we need\n",
    "$$\n",
    "\\partial_\\theta P(\\theta, \\phi|\\mathbf{w})\n",
    "=\n",
    "\\partial_\\theta \\sum_{z}P(\\theta, \\phi|\\mathbf{z}, \\mathbf{w}) P(\\mathbf{z}|\\mathbf{w})\n",
    "=\n",
    "\\sum_{z} P(\\mathbf{z}|\\mathbf{w}) \\partial_\\theta P(\\theta, \\phi|\\mathbf{z}, \\mathbf{w})\n",
    "=0\n",
    "$$\n",
    "\n",
    "similarly\n",
    "$$\n",
    "\\sum_{z} P(\\mathbf{z}|\\mathbf{w}) \\partial_\\phi P(\\theta, \\phi|\\mathbf{z}, \\mathbf{w})\n",
    "=0\n",
    "$$\n",
    "\n",
    "\n",
    "We can sample $z$ with probability of $P(\\mathbf{z}|\\mathbf{w})$, and move $\\theta$ and $\\phi$ along the direction of \n",
    "$-\\partial_\\theta P(\\theta, \\phi|\\mathbf{z}, \\mathbf{w})$ and $-\\partial_\\phi P(\\theta, \\phi, \\mathbf{z}|\\mathbf{w})$, respectively.\n",
    "(At least) for small engouh the step, we will reach the statination point.\n",
    "But it's often bad to use gradient decent method with probablity (we should use log(probablity)). \n",
    "\n",
    "we write it in the form of \n",
    "$$\n",
    "P(\\theta, \\phi|\\mathbf{w}) \n",
    "=\n",
    "\\sum_{z} P(\\theta, \\phi, \\mathbf{z}, \\mathbf{w})/P(\\mathbf{w})\n",
    "=\n",
    "\\sum_{z} P(\\mathbf{z}, \\mathbf{w} | \\theta, \\phi)P(\\theta,\\phi)/P(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "### the pLSA\n",
    "EM algoirhtm, we define the Q function as\n",
    "$$\n",
    "\\sum_{z} P(\\mathbf{z} | \\mathbf{w}, \\theta^i, \\phi^i) \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)\n",
    "$$\n",
    "We to maximize to Q (with constraint of $\\sum_k \\theta_{mk}=1$, or normalize in time)\n",
    "$$\n",
    "\\partial_\\theta \\sum_{z}  P(\\mathbf{z} | \\mathbf{w}, \\theta^i, \\phi^i) \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi) = 0\n",
    "$$\n",
    "$$\n",
    "\\sum_{z}  P(\\mathbf{z} | \\mathbf{w}, \\theta^i, \\phi^i) \\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "We can sample $z$ with probability of $ P(\\mathbf{z} | \\mathbf{w}, \\theta^i, \\phi^i)$, and move $\\theta$ and $\\phi$ along the direction of \n",
    "$-\\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)$ and $-\\partial_\\phi \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)$, respectively.\n",
    "The update ruler\n",
    "$$\n",
    "\\theta^{i+1} = \\theta + \\eta \\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)\n",
    "$$\n",
    "$$\n",
    "\\phi^{i+1} = \\phi + \\eta \\partial_\\phi\\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)\n",
    "$$\n",
    "\n",
    "It's simple that\n",
    "$$\n",
    "\\log P(\\mathbf{z},\\mathbf{w}|\\theta,\\phi)\n",
    "=\n",
    "\\prod_{mn} \\text{Cat}(z_{mn}|\\theta_m) \n",
    "\\text{Cat}(w_{mn}|\\phi_{z_{mn}})\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\partial_{\\theta_m} \\log P(\\mathbf{z},\\mathbf{w}|\\theta,\\phi)\n",
    "=\n",
    "n_m/\\theta_m\n",
    "$$\n",
    "$$\n",
    "\\partial_{\\phi_k} \\log P(\\mathbf{z},\\mathbf{w}|\\theta,\\phi)\n",
    "=\n",
    "n_k/\\phi_k\n",
    "$$\n",
    "\n",
    "Excisie:\n",
    "Note the normalized pdf of $\\mathbf{z}$ and $\\mathbf{w}$ should be\n",
    "$$\n",
    "P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)\n",
    " =\n",
    "P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)/P(\\theta, \\phi)\n",
    " =\n",
    "P(\\theta, \\phi | \\mathbf{z}, \\mathbf{w}) P(\\mathbf{z}, \\mathbf{w})/P(\\theta, \\phi)\n",
    "$$\n",
    "\n",
    "we just recall\n",
    "$$\n",
    "P(\\theta, \\phi)\n",
    "=\n",
    "\\left[\\prod_k \\text{Dir} (\\phi_k|\\beta)\\right]\n",
    "\\left[\\prod_m \\text{Dir}(\\theta_m|\\alpha)\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log P(\\mathbf{z},\\mathbf{w}|\\theta,\\phi)\n",
    "= \n",
    "\\log P(\\mathbf{z}, \\mathbf{w})\n",
    "+ \\log \\left[\\prod_m \\text{Dir}(\\theta_m|n_m+\\alpha)\\right]\n",
    "+\\log\\left[\\prod_k \\text{Dir}(\\phi_k|n_k+\\beta)\\right]\n",
    "-\\log \\left[\\prod_m \\text{Dir}(\\theta_m|\\alpha)\\right]\n",
    "-\\log \\left[\\prod_k \\text{Dir} (\\phi_k|\\beta)\\right]\n",
    "$$\n",
    "we can again obtain the $P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)$\n",
    "\n",
    "and \n",
    "$$\n",
    "P(\\mathbf{z} |\\mathbf{w}, \\theta, \\phi)\n",
    "=\n",
    "P(\\mathbf{z}, \\mathbf{w} | \\theta, \\phi)/ P(\\mathbf{w}|\\theta, \\phi)\n",
    "\\propto\n",
    "\\theta_{mk} \\phi_{kn}\n",
    "$$\n",
    "for the document $m$ and word $n$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### the LDA\n",
    "Like EM algoirhtm, we define the Q function as\n",
    "$$\n",
    "\\sum_{z} P(\\mathbf{z}, \\mathbf{w}, \\theta^i, \\phi^i) \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)\n",
    "$$\n",
    "We to maximize to Q (with constraint of $\\sum_k \\theta_{mk}=1$, or normalize in time)\n",
    "$$\n",
    "\\partial_\\theta \\sum_{z}  P(\\mathbf{z}, \\mathbf{w}, \\theta^i, \\phi^i) \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi) = 0\n",
    "$$\n",
    "$$\n",
    "\\sum_{z}  P(\\mathbf{z}, \\mathbf{w}, \\theta^i, \\phi^i) \\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "We can sample $z$ with probability of $ P(\\mathbf{z}, \\mathbf{w}, \\theta^i, \\phi^i)$, and move $\\theta$ and $\\phi$ along the direction of \n",
    "$-\\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)$ and $-\\partial_\\phi \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)$, respectively.\n",
    "The update ruler\n",
    "$$\n",
    "\\theta^{i+1} = \\theta + \\eta \\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)\n",
    "$$\n",
    "$$\n",
    "\\phi^{i+1} = \\phi + \\eta \\partial_\\phi\\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)\n",
    "$$\n",
    "\n",
    "It's simple that\n",
    "$$\n",
    "\\partial_{\\theta_m} \\log P(\\mathbf{z},\\mathbf{w}, \\theta,\\phi)\n",
    "=\n",
    "(n_m + \\alpha - 1)/\\theta_m\n",
    "$$\n",
    "$$\n",
    "\\partial_{\\phi_k} \\log P(\\mathbf{z},\\mathbf{w}, \\theta,\\phi)\n",
    "=\n",
    "(n_k + \\beta - 1)/\\phi_k\n",
    "$$\n",
    "\n",
    "and \n",
    "$$\n",
    "P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)\n",
    "\\propto\n",
    "\\theta_{mk} \\phi_{kn}\n",
    "$$\n",
    "for the document $m$ and word $n$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'terms_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-6ed90597dfcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtheta_mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntopics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtheta_mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta_mk\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtheta_mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mphi_kv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mntopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterms_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mphi_kv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphi_kv\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mphi_kv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'terms_vec' is not defined"
     ]
    }
   ],
   "source": [
    "alpha=1.0\n",
    "beta=1.0\n",
    "\n",
    "theta_mk = np.random.rand(len(documents), ntopics)\n",
    "theta_mk = theta_mk / theta_mk.sum(axis=1, keepdims=True)\n",
    "phi_kv = np.random.rand(ntopics, len(terms_vec))\n",
    "phi_kv = phi_kv / phi_kv.sum(axis=1, keepdims=True)\n",
    "\n",
    "def zwtp_Gibbs_topic_prob(m, v):    \n",
    "    return theta_mk[m,:] * phi_kv[:,v]\n",
    "\n",
    "def zwtp_MH_topic_prob(m, v, k):\n",
    "    return theta_mk[m, k] * phi_kv[k, v]\n",
    "\n",
    "intialize_randomly()\n",
    "\n",
    "# warnming\n",
    "for _ in range(1000):\n",
    "    Gibbs_step(zwtp_Gibbs_topic_prob)\n",
    "\n",
    "def normalize(mat):\n",
    "    return mat/mat.sum(axis=1, keepdims=True)\n",
    "    \n",
    "eta = 0.01\n",
    "for _ in range(1000):\n",
    "    MH_step(zwtp_MH_topic_prob)\n",
    "    \n",
    "    #print(n_mk[:,:] + alpha - 1)\n",
    "    #print(theta_mk)\n",
    "    d = (n_mk[:,:] + alpha - 1)/theta_mk[:,:]\n",
    "    theta_mk = theta_mk + eta*d\n",
    "    theta_mk = normalize(theta_mk)\n",
    "    \n",
    "    d = (n_kv[:,:] + beta - 1)/phi_kv[:,:]\n",
    "    phi_kv = phi_kv + eta*d\n",
    "    phi_kv = normalize(phi_kv)\n",
    "    \n",
    "    #print(theta_mk)\n",
    "    #print(phi_kv)\n",
    "    #print_mat(n_kv.transpose(), n_mk.transpose())\n",
    "    #print(theta_mk.sum(axis=1))\n",
    "    #print(phi_kv.sum(axis=1))\n",
    "    #print(k_mn)\n",
    "    #print(n_kv)\n",
    "    #print(n_mk)\n",
    "    \n",
    "print_mat(phi_kv.transpose(), theta_mk.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2][0:2]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "24",
    "lenType": "16",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
