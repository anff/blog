{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#潜在语义模型练习\" data-toc-modified-id=\"潜在语义模型练习-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>潜在语义模型练习</a></span></li><li><span><a href=\"#潜在语义分析（LSA）\" data-toc-modified-id=\"潜在语义分析（LSA）-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>潜在语义分析（LSA）</a></span><ul class=\"toc-item\"><li><span><a href=\"#奇异值分解（SVD）\" data-toc-modified-id=\"奇异值分解（SVD）-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>奇异值分解（SVD）</a></span></li><li><span><a href=\"#非负矩阵分解\" data-toc-modified-id=\"非负矩阵分解-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>非负矩阵分解</a></span></li></ul></li><li><span><a href=\"#概率潜在语义分析(pLSA)\" data-toc-modified-id=\"概率潜在语义分析(pLSA)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>概率潜在语义分析(pLSA)</a></span></li><li><span><a href=\"#潜在狄利克雷分配(LDA)\" data-toc-modified-id=\"潜在狄利克雷分配(LDA)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>潜在狄利克雷分配(LDA)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gibbs抽样\" data-toc-modified-id=\"Gibbs抽样-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Gibbs抽样</a></span></li><li><span><a href=\"#随机梯度下降-(Stochastic-gradient-descent)\" data-toc-modified-id=\"随机梯度下降-(Stochastic-gradient-descent)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>随机梯度下降 (Stochastic gradient descent)</a></span><ul class=\"toc-item\"><li><span><a href=\"#the-pLSA\" data-toc-modified-id=\"the-pLSA-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>the pLSA</a></span></li><li><span><a href=\"#the-LDA\" data-toc-modified-id=\"the-LDA-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>the LDA</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 潜在语义模型练习\n",
    "\n",
    "主要参考赵航的书"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 潜在语义分析（LSA）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的文档包含两个主题：一个是粒子物理，常用词汇为\"meson\", \"photon\"。\n",
    "另外一个是生活，常用词汇为\"girl\", \"boy\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 terms found in 12 documents.\n",
      "terms: ['girl', 'love', 'boy', 'meson', 'hits', 'photon']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# documents set\n",
    "documents=[\n",
    "    \"girl love boy\",\n",
    "    \"boy love girl\",\n",
    "    \"girl love girl\",\n",
    "    \"boy love boy\",\n",
    "    \n",
    "    \"meson hits photon\",\n",
    "    \"photon hits meson\",\n",
    "    \"photon hits photon\",\n",
    "    \"meson hits meson\",\n",
    "\n",
    "    \"boy love meson\",\n",
    "    \"girl love photon\",\n",
    "    \"meson hits girl\",\n",
    "    \"photon hits boy\",\n",
    "]\n",
    "\n",
    "# setup terms set\n",
    "terms_vec=[]\n",
    "terms_set = set()\n",
    "for doc in documents:\n",
    "    terms = doc.split()\n",
    "    for term in terms:\n",
    "        if term not in terms_set:\n",
    "            terms_vec.append(term)\n",
    "            terms_set.add(term)\n",
    "        \n",
    "        \n",
    "print(\"%d terms found in %d documents.\"%(len(terms_set), len(documents)))\n",
    "print(\"terms:\", terms_vec)\n",
    "\n",
    "\n",
    "# setup terms index\n",
    "terms_index = { }\n",
    "for idx, term in enumerate(terms_set):\n",
    "    terms_index[term] = idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用频率逆文档(TFIDF)来表示term-document矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term-document (6X12) matrix setup\n",
      "[[0.    0.    0.    0.    0.292 0.292 0.    0.584 0.292 0.    0.292 0.   ]\n",
      " [0.292 0.292 0.584 0.    0.    0.    0.    0.    0.    0.292 0.292 0.   ]\n",
      " [0.    0.    0.    0.    0.231 0.231 0.231 0.231 0.    0.    0.231 0.231]\n",
      " [0.292 0.292 0.    0.584 0.    0.    0.    0.    0.292 0.    0.    0.292]\n",
      " [0.    0.    0.    0.    0.292 0.292 0.584 0.    0.    0.292 0.    0.292]\n",
      " [0.231 0.231 0.231 0.231 0.    0.    0.    0.    0.231 0.231 0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# terms-documents matrix\n",
    "tf_ij=np.zeros((len(terms_set),len(documents)))\n",
    "tf_j=np.zeros(len(documents))  # number of terms in document j \n",
    "df_i=np.zeros(len(terms_set))  # number of documents which contain term i\n",
    "df=len(documents)\n",
    "\n",
    "\n",
    "term_doc_mat = np.zeros([len(terms_set), len(documents)])\n",
    "for j, doc in enumerate(documents):\n",
    "    terms = doc.split()\n",
    "    tf_j[j] = len(terms)\n",
    "    \n",
    "    add_to_df_i=np.zeros(len(terms_set))\n",
    "    for term in terms:\n",
    "        i = terms_index[term]\n",
    "        tf_ij[i][j] = tf_ij[i][j] + 1\n",
    "        add_to_df_i[i] = 1\n",
    "        \n",
    "    df_i = df_i + add_to_df_i\n",
    "\n",
    "\n",
    "# X in hang's book\n",
    "term_doc_mat = tf_ij/tf_j*np.log(df/df_i).reshape(df_i.shape[0], 1)\n",
    "        \n",
    "with np.printoptions(precision=3):\n",
    "    print(\"term-document (%dX%d) matrix setup\"%(term_doc_mat.shape[0],term_doc_mat.shape[1]))\n",
    "    print(term_doc_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 奇异值分解（SVD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将term-documents矩阵分解为 u s vT\n",
    "保留其中主值最大的部分。\n",
    "奇异值分解的正交性要求，矩阵的部分数值小于0，如何解释呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc-topic matrix\n",
      "                            topic0      topic1\n",
      "`       girl love boy'   -0.288675   -0.347652\n",
      "`       boy love girl'   -0.288675   -0.347652\n",
      "`      girl love girl'   -0.288675   -0.347652\n",
      "`        boy love boy'   -0.288675   -0.347652\n",
      "`   meson hits photon'   -0.288675   +0.347652\n",
      "`   photon hits meson'   -0.288675   +0.347652\n",
      "`  photon hits photon'   -0.288675   +0.347652\n",
      "`    meson hits meson'   -0.288675   +0.347652\n",
      "`      boy love meson'   -0.288675   -0.090976\n",
      "`    girl love photon'   -0.288675   -0.090976\n",
      "`     meson hits girl'   -0.288675   +0.090976\n",
      "`     photon hits boy'   -0.288675   +0.090976\n",
      "term-topic matrix\n",
      "                            topic0      topic1\n",
      "                 girl    -0.436281   +0.422454\n",
      "                 love    -0.436281   -0.422454\n",
      "                  boy    -0.345423   +0.378239\n",
      "                meson    -0.436281   -0.422454\n",
      "                 hits    -0.436281   +0.422454\n",
      "               photon    -0.345423   -0.378239\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "u,s,vT = svd(term_doc_mat)\n",
    "\n",
    "#print(\"term-topic matrix:\\n\", u)\n",
    "#print(\"topic-document matrix:\\n\", vT)\n",
    "#print(\"main value:\\n\", s)\n",
    "\n",
    "def print_mat(term_topic_mat, topic_document_mat):\n",
    "    print(\"doc-topic matrix\")\n",
    "    print(\" %20s   %10s  %10s\"%(\"\",\"topic0\", \"topic1\"))\n",
    "    for i, doc in enumerate(documents):\n",
    "        print(\"`%20s'  %+10f  %+10f\"%(doc, topic_document_mat[0][i], topic_document_mat[1][i]))\n",
    "\n",
    "    print(\"term-topic matrix\")\n",
    "    print(\" %20s   %10s  %10s\"%(\"\",\"topic0\", \"topic1\"))\n",
    "\n",
    "    for i, term in enumerate(terms_vec):\n",
    "        print(\" %20s   %+10f  %+10f\"%(term, term_topic_mat[i][0], term_topic_mat[i][1]))\n",
    "\n",
    "print_mat(u, vT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非负矩阵分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将term-documents矩阵$X$分解为 $WH$，其中$W$和$H$为（任意元素）非负矩阵。\n",
    "目标为优化\n",
    "$$\\sum_{ij}(WH-X)_{ij}^2$$\n",
    "优化方法采用“乘法更新规则”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want 2 topics\n",
      "doc-topic matrix\n",
      "                            topic0      topic1\n",
      "`       girl love boy'   +0.000000   +0.215870\n",
      "`       boy love girl'   +0.000000   +0.215870\n",
      "`      girl love girl'   +0.000000   +0.215869\n",
      "`        boy love boy'   +0.000000   +0.215870\n",
      "`   meson hits photon'   +0.253443   +0.000000\n",
      "`   photon hits meson'   +0.253443   +0.000000\n",
      "`  photon hits photon'   +0.253443   +0.000000\n",
      "`    meson hits meson'   +0.253443   +0.000000\n",
      "`      boy love meson'   +0.092725   +0.134097\n",
      "`    girl love photon'   +0.092728   +0.134095\n",
      "`     meson hits girl'   +0.157436   +0.078981\n",
      "`     photon hits boy'   +0.157434   +0.078980\n",
      "term-topic matrix\n",
      "                            topic0      topic1\n",
      "                 girl    +1.135731   +0.024257\n",
      "                 love    +0.020654   +1.333411\n",
      "                  boy    +0.948347   +0.000000\n",
      "                meson    +0.020633   +1.333420\n",
      "                 hits    +1.135736   +0.024242\n",
      "               photon    +0.000000   +1.113411\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "ntopics = 2\n",
    "print(\"We want %d topics\"%ntopics)\n",
    "\n",
    "W = np.random.rand(len(terms_vec), ntopics)\n",
    "H = np.random.rand(ntopics, len(documents))\n",
    "# W * H -> X (term-document matrix)\n",
    "for i in range(100):\n",
    "    H1 = H * ((W.transpose().dot(term_doc_mat)) / (W.transpose().dot(W).dot(H)))\n",
    "    dH = H1 - H\n",
    "    H = H1\n",
    "    W1 = W*((term_doc_mat.dot(H.transpose())) / (W.dot(H.dot(H.transpose()))))\n",
    "    dW = W1 - W\n",
    "    W = W1\n",
    "    #print(np.power(dH,2).sum())\n",
    "    #print(np.power(dW,2).sum())\n",
    "    #print(np.power(term_doc_mat-W.dot(H),2).sum())\n",
    "\n",
    "print_mat(W, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概率潜在语义分析(pLSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概率潜在语义分析即潜在语义分析概率化版本，是一种生成模型。\n",
    "\n",
    "设文档集合为$D=\\{d_j\\}, j < |D|$，单词集合为$W=\\{w_i\\}, i < |W|$，文档$d_j$也是单词的序列，序列长度为$|d_j|$（重复单词多次计数。）\n",
    "\n",
    "概率模型给出文档$d_j$中某一单词位置（比如第一个单词），单词$w_i$出现的概率$P(w_i, d_j)$为\n",
    "$$\n",
    "P(w_i, d_j) = \n",
    "\\sum_k P(w_i, z_k) P(z_k, d_j)\n",
    "$$\n",
    "其中$P(z_k, d_j)$为文档$d_j$中某一单词选取话题$z_k$的概率，$P(w_i, z_k)$为话题$z_k$中单词$w_i$出现的概率。\n",
    "\n",
    "设文档$d_j$第$i^\\prime$个单词是单词$w_{i(d_j, i^\\prime)}$（$i^\\prime < |d_j|$），\n",
    "则文档$d_j$生成的概率为\n",
    "$$\n",
    "\\prod_{i^\\prime < |d_j|} P(w_{i(d_j, i^\\prime)}, d_j)\n",
    "=\\prod_{i < |W|} { P(w_i, d_j)^{n(w_i, d_j)} }\n",
    "$$\n",
    "$n(w_i, d_j)$为单词$i$在文档$j$中出现的频次。\n",
    "所有文档的生成概率的对数为\n",
    "$$\n",
    "L = \\sum_j \\sum_{i} n(w_i, d_j) \\log P(w_i, d_j)\n",
    "$$\n",
    "优化方法采用EM算法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want 2 topics\n",
      "doc-topic matrix\n",
      "                            topic0      topic1\n",
      "`       girl love boy'   +0.999997   +0.000000\n",
      "`       boy love girl'   +0.999997   +0.000000\n",
      "`      girl love girl'   +0.999997   +0.000000\n",
      "`        boy love boy'   +0.999997   +0.000000\n",
      "`   meson hits photon'   +0.000000   +0.999997\n",
      "`   photon hits meson'   +0.000000   +0.999997\n",
      "`  photon hits photon'   +0.000000   +0.999997\n",
      "`    meson hits meson'   +0.000000   +0.999997\n",
      "`      boy love meson'   +0.666664   +0.333330\n",
      "`    girl love photon'   +0.666664   +0.333330\n",
      "`     meson hits girl'   +0.333330   +0.666664\n",
      "`     photon hits boy'   +0.333330   +0.666664\n",
      "term-topic matrix\n",
      "                            topic0      topic1\n",
      "                 girl    +0.000000   +0.333333\n",
      "                 love    +0.333333   +0.000000\n",
      "                  boy    +0.000000   +0.333334\n",
      "                meson    +0.333333   +0.000000\n",
      "                 hits    +0.000000   +0.333333\n",
      "               photon    +0.333334   +0.000000\n"
     ]
    }
   ],
   "source": [
    "ntopics = 2\n",
    "print(\"We want %d topics\"%ntopics)\n",
    "\n",
    "\n",
    "Pwz = np.random.rand(len(terms_vec), ntopics)\n",
    "Pzd = np.random.rand(ntopics, len(documents))\n",
    "\n",
    "nwd = np.zeros((len(terms_vec), len(documents))) # number of word `w` in document `d`\n",
    "\n",
    "# setup terms set\n",
    "for j, doc in enumerate(documents):\n",
    "    terms = doc.split()\n",
    "    for term in terms:\n",
    "        i = terms_index[term]\n",
    "        nwd[i][j] = nwd[i][j] + 1\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    \n",
    "    \n",
    "    nd = nwd.sum(axis=0)\n",
    "    \n",
    "    # add 1E-6 to avoid 0\n",
    "    # we need to inverse the elements of this matrix\n",
    "    Pwd = Pwz.dot(Pzd) + np.full((len(terms_vec),len(documents)), 1E-6)\n",
    "    \n",
    "    Pwz_new = (nwd/Pwd).dot(Pzd.transpose()) * Pwz\n",
    "    Pwz_new = Pwz_new / Pwz_new.sum(axis=0)\n",
    "    \n",
    "    \n",
    "    Pzd_new = (nwd/Pwd).transpose().dot(Pwz).transpose() * Pzd\n",
    "    Pzd_new = Pzd_new / nd\n",
    "        \n",
    "    #print(np.power(Pwz_new-Pwz,2).sum())\n",
    "    #print(np.power(Pzd_new-Pzd,2).sum())\n",
    "    #print(np.power(Pzd_new,2).sum())\n",
    "    \n",
    "    Pwz = Pwz_new\n",
    "    Pzd = Pzd_new\n",
    "\n",
    "\n",
    "print_mat(Pwz, Pzd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 潜在狄利克雷分配(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设文档集合为$D=\\{d_m\\}, m<|D|$，单词集合为$W=\\{w_i\\},i < |W|$，文档$d_j$也是单词的序列，序列长度为$|d_m|$（重复单词多次计数。）。\n",
    "话题向量为$\\mathbf{z}=\\{z_m\\},m < |D|$。\n",
    "单词向量为$\\mathbf{w} = \\{w_{mn}\\}, m < |D|, n< |d_m|$。\n",
    "话题向量为$\\mathbf{z} = \\{z_{mn}\\}, m < |D|, n< |d_m|$。\n",
    "话题向量先验计数为$\\mathbf{\\alpha} = \\{\\alpha_{mk}\\}, m < |D|, k < K$。\n",
    "话题词向量先验计数为$\\mathbf{\\beta} = \\{\\beta_{ki}\\}, k < K, i < |W|$。\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\mathbf{w}, \\mathbf{z}, \\mathbf{\\theta},\\mathbf{\\phi}| \\alpha, \\beta) = \n",
    "\\left[\\prod_k \\text{Dir} (\\phi_k|\\beta)\\right]\n",
    "\\left[\\prod_m \\text{Dir}(\\theta_m|\\alpha)\\right]\n",
    "\\left[\n",
    "\\prod_{mn} \\text{Cat}(z_{mn}|\\theta_m) \n",
    "\\text{Cat}(w_{mn}|\\phi_{z_{mn}})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "我们想要求 $P(\\mathbf{z},\\theta,\\phi|\\mathbf{w})$\n",
    "$$P(\\mathbf{z},\\theta,\\phi|\\mathbf{w})\n",
    "=\n",
    "P(\\theta,\\phi|\\mathbf{z},\\mathbf{w})\n",
    "P(\\mathbf{z},\\mathbf{w})\n",
    "/P(\\mathbf{w})\n",
    "=\n",
    "P(\\theta,\\phi|\\mathbf{z},\\mathbf{w})\n",
    "P(\\mathbf{z}|\\mathbf{w})\n",
    "$$\n",
    "其中\n",
    "\n",
    "$$\n",
    "P(\\mathbf{z},\\mathbf{w}) = \\prod_{k} \\frac{B(n_k+\\beta)}{B(\\beta)} \\prod_{m} \\frac{B(n_m+\\alpha)}{B(\\alpha)}\n",
    "$$\n",
    "\n",
    "和\n",
    "\n",
    "$$\n",
    "P(\\theta,\\phi|\\mathbf{z},\\mathbf{w})\n",
    "= \\left[\\prod_m \\text{Dir}(\\theta_m|n_m+\\alpha)\\right] \\left[\\prod_k \\text{Dir}(\\phi_k|n_k+\\beta)\\right]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs抽样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\mathbf{z}|\\mathbf{w})$可以通过Gibbs抽样方法得到。\n",
    "注意根据狄利克雷分布，\n",
    "$\\theta_m$ 的期望为 $$(n_m + \\alpha)/\\sum_k (n_{mk}+\\alpha_{mk})$$\n",
    "$\\phi_k$ 的期望为 $$(n_k + \\beta)/\\sum_{i} (n_{ki}+\\beta_{ki})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want 2 topics\n",
      "doc-topic matrix\n",
      "                            topic0      topic1\n",
      "`       girl love boy'   +0.800000   +0.200000\n",
      "`       boy love girl'   +0.800000   +0.200000\n",
      "`      girl love girl'   +0.800000   +0.200000\n",
      "`        boy love boy'   +0.800000   +0.200000\n",
      "`   meson hits photon'   +0.200000   +0.800000\n",
      "`   photon hits meson'   +0.400000   +0.600000\n",
      "`  photon hits photon'   +0.400000   +0.600000\n",
      "`    meson hits meson'   +0.200000   +0.800000\n",
      "`      boy love meson'   +0.600000   +0.400000\n",
      "`    girl love photon'   +0.600000   +0.400000\n",
      "`     meson hits girl'   +0.200000   +0.800000\n",
      "`     photon hits boy'   +0.200000   +0.800000\n",
      "term-topic matrix\n",
      "                            topic0      topic1\n",
      "                 girl    +0.041667   +0.291667\n",
      "                 love    +0.250000   +0.083333\n",
      "                  boy    +0.041667   +0.291667\n",
      "                meson    +0.250000   +0.083333\n",
      "                 hits    +0.125000   +0.208333\n",
      "               photon    +0.291667   +0.041667\n"
     ]
    }
   ],
   "source": [
    "ntopics = 2\n",
    "print(\"We want %d topics\"%ntopics)\n",
    "\n",
    "alpha=1\n",
    "beta=1\n",
    "\n",
    "n_mk=np.zeros((len(documents), ntopics))\n",
    "n_kv=np.zeros((ntopics, len(terms_vec)))\n",
    "n_m=np.zeros(len(documents))\n",
    "n_k=np.zeros(ntopics)\n",
    "\n",
    "doc_lengths=np.zeros(len(documents), dtype=np.int)\n",
    "for m, doc in enumerate(documents):\n",
    "    doc_lengths[m] = len(doc.split())    \n",
    "\n",
    "k_mn=np.zeros((len(documents), np.max(doc_lengths)),dtype=np.int)\n",
    "v_mn=np.zeros((len(documents), np.max(doc_lengths)),dtype=np.int)\n",
    "\n",
    "for m, doc in enumerate(documents):\n",
    "    terms = doc.split()\n",
    "    for n, term in enumerate(terms):\n",
    "        v = terms_index[term]\n",
    "        v_mn[m, n] = v\n",
    "\n",
    "\n",
    "# intialization randomly\n",
    "# assign each word a topic\n",
    "def intialize_randomly():\n",
    "    n_mk.fill(0)\n",
    "    n_m.fill(0)\n",
    "    n_kv.fill(0)\n",
    "    n_k.fill(0)\n",
    "    k_mn.fill(0)\n",
    "    \n",
    "    for m in range(len(documents)):\n",
    "        for n in range(doc_lengths[m]):\n",
    "            v = v_mn[m, n]\n",
    "            k = np.random.randint(ntopics, dtype=np.int)        \n",
    "            n_mk[m, k] += 1 \n",
    "            n_m[m] += 1 \n",
    "            n_kv[k,v] += 1 \n",
    "            n_k[k] += 1\n",
    "            k_mn[m, n] = k\n",
    "        \n",
    "# generate P(z|w) sample\n",
    "def zw_Gibbs_topic_prob(m, v):\n",
    "    n_kv_ = (n_kv[:,v] + beta) / (n_kv[:,:] + beta).sum(axis=1)\n",
    "    n_mk_ = (n_mk[m,:] + alpha) / (n_mk[m,:] + alpha).sum()\n",
    "    return n_kv_*n_mk_\n",
    "\n",
    "# generate P(z|w) sample\n",
    "def zw_MH_topic_prob(m, v, k):\n",
    "    n_kv_ = (n_kv[k,v] + beta) / (n_kv[k,:] + beta).sum()\n",
    "    n_mk_ = (n_mk[m,k] + alpha) / (n_mk[m,:] + alpha).sum()\n",
    "    return n_kv_*n_mk_\n",
    "\n",
    "def MH_step(topic_prob):\n",
    "    \n",
    "    m = np.random.randint(len(documents), dtype=np.int)\n",
    "    n = np.random.randint(doc_lengths[m], dtype=np.int)\n",
    "\n",
    "    v = v_mn[m, n]\n",
    "    k = k_mn[m, n]          \n",
    "    kp = np.random.randint(ntopics, dtype=np.int)\n",
    "    \n",
    "    k_prob = topic_prob(m, v, k)\n",
    "    kp_prob = topic_prob(m, v, kp)\n",
    "    \n",
    "    acc = kp_prob/(k_prob+1E-10)\n",
    "    u = np.random.rand()\n",
    "    if u < acc:\n",
    "        # accept      \n",
    "        n_mk[m, k] -= 1\n",
    "        #n_m[m] -= 1\n",
    "        n_kv[k, v] -= 1\n",
    "        n_k[k] -= 1\n",
    "\n",
    "        k_mn[m, n] = kp\n",
    "        n_mk[m, kp] += 1\n",
    "        #n_m[m] += 1\n",
    "        n_kv[kp, v] += 1\n",
    "        n_k[kp] += 1\n",
    "    \n",
    "    \n",
    "def Gibbs_step(sample_topic):\n",
    "\n",
    "    for m in range(len(documents)):\n",
    "        for n in range(doc_lengths[m]):\n",
    "            v = v_mn[m, n]\n",
    "\n",
    "            # current topic\n",
    "            k = k_mn[m, n]            \n",
    "            n_mk[m, k] -= 1\n",
    "            #n_m[m] -= 1\n",
    "            n_kv[k, v] -= 1\n",
    "            n_k[k] -= 1\n",
    "            \n",
    "            topics_prob = sample_topic(m, v)\n",
    "            acc = topics_prob.cumsum()/topics_prob.sum()\n",
    "            p = np.random.rand()\n",
    "            kp = np.searchsorted(acc, p, side=\"left\")\n",
    "                        \n",
    "            k_mn[m, n] = kp\n",
    "            n_mk[m, kp] += 1\n",
    "            #n_m[m] += 1\n",
    "            n_kv[kp, v] += 1\n",
    "            n_k[kp] += 1\n",
    "\n",
    "\n",
    "def print_matrix():\n",
    "    print(\"doc-topic counting\")\n",
    "    print(n_mk)\n",
    "    print(\"terms counting in each doc\")\n",
    "    print(n_m)\n",
    "    print(\"document length\")\n",
    "    print(lengths)\n",
    "    print(\"topic-words counting\")\n",
    "    print(n_kv)\n",
    "    print(\"terms counting in each topic\")\n",
    "    print(n_k)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "intialize_randomly()\n",
    "            \n",
    "# warnming\n",
    "for _ in range(1000):\n",
    "    Gibbs_step(zw_Gibbs_topic_prob)\n",
    "\n",
    "    \n",
    "iters=5\n",
    "theta_mk_iters=np.zeros((iters, len(documents), ntopics))\n",
    "phi_kv_iters=np.zeros((iters, ntopics, len(terms_vec)))\n",
    "    \n",
    "\n",
    "# average\n",
    "for it in range(iters):\n",
    "    #Gibbs_step(zw_sample_topic)\n",
    "    MH_step(zw_MH_topic_prob)\n",
    "    \n",
    "    theta_mk = (n_mk + alpha)/(n_mk + alpha).sum(axis=1, keepdims=True)\n",
    "    phi_kv = (n_kv + beta)/(n_kv + beta).sum(axis=1, keepdims=True)\n",
    "    #print_mat(phi_kv.transpose(), theta_mk.transpose())\n",
    "\n",
    "    theta_mk_iters[it,:,:] = theta_mk\n",
    "    phi_kv_iters[it,:,:] = phi_kv\n",
    "    \n",
    "    \n",
    "print_mat(phi_kv_iters.mean(axis=0).transpose(), theta_mk_iters.mean(axis=0).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降 (Stochastic gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to maximize\n",
    "$$\n",
    "P(\\theta, \\phi|\\mathbf{w}) = \\sum_{z}P(\\theta, \\phi|\\mathbf{z}, \\mathbf{w}) P(\\mathbf{z}|\\mathbf{w})\n",
    "$$\n",
    "\n",
    "then we need\n",
    "$$\n",
    "\\partial_\\theta P(\\theta, \\phi|\\mathbf{w})\n",
    "=\n",
    "\\partial_\\theta \\sum_{z}P(\\theta, \\phi|\\mathbf{z}, \\mathbf{w}) P(\\mathbf{z}|\\mathbf{w})\n",
    "=\n",
    "\\sum_{z} P(\\mathbf{z}|\\mathbf{w}) \\partial_\\theta P(\\theta, \\phi|\\mathbf{z}, \\mathbf{w})\n",
    "=0\n",
    "$$\n",
    "\n",
    "similarly\n",
    "$$\n",
    "\\sum_{z} P(\\mathbf{z}|\\mathbf{w}) \\partial_\\phi P(\\theta, \\phi|\\mathbf{z}, \\mathbf{w})\n",
    "=0\n",
    "$$\n",
    "\n",
    "\n",
    "We can sample $z$ with probability of $P(\\mathbf{z}|\\mathbf{w})$, and move $\\theta$ and $\\phi$ along the direction of \n",
    "$-\\partial_\\theta P(\\theta, \\phi|\\mathbf{z}, \\mathbf{w})$ and $-\\partial_\\phi P(\\theta, \\phi, \\mathbf{z}|\\mathbf{w})$, respectively.\n",
    "(At least) for small engouh the step, we will reach the statination point.\n",
    "But it's often bad to use gradient decent method with probablity (we should use log(probablity)). \n",
    "\n",
    "we write it in the form of \n",
    "$$\n",
    "P(\\theta, \\phi|\\mathbf{w}) \n",
    "=\n",
    "\\sum_{z} P(\\theta, \\phi, \\mathbf{z}, \\mathbf{w})/P(\\mathbf{w})\n",
    "=\n",
    "\\sum_{z} P(\\mathbf{z}, \\mathbf{w} | \\theta, \\phi)P(\\theta,\\phi)/P(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "### the pLSA\n",
    "EM algoirhtm, we define the Q function as\n",
    "$$\n",
    "\\sum_{z} P(\\mathbf{z} | \\mathbf{w}, \\theta^i, \\phi^i) \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)\n",
    "$$\n",
    "We to maximize to Q (with constraint of $\\sum_k \\theta_{mk}=1$, or normalize in time)\n",
    "$$\n",
    "\\partial_\\theta \\sum_{z}  P(\\mathbf{z} | \\mathbf{w}, \\theta^i, \\phi^i) \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi) = 0\n",
    "$$\n",
    "$$\n",
    "\\sum_{z}  P(\\mathbf{z} | \\mathbf{w}, \\theta^i, \\phi^i) \\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "We can sample $z$ with probability of $ P(\\mathbf{z} | \\mathbf{w}, \\theta^i, \\phi^i)$, and move $\\theta$ and $\\phi$ along the direction of \n",
    "$-\\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)$ and $-\\partial_\\phi \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)$, respectively.\n",
    "The update ruler\n",
    "$$\n",
    "\\theta^{i+1} = \\theta + \\eta \\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)\n",
    "$$\n",
    "$$\n",
    "\\phi^{i+1} = \\phi + \\eta \\partial_\\phi\\log  P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)\n",
    "$$\n",
    "\n",
    "It's simple that\n",
    "$$\n",
    "\\log P(\\mathbf{z},\\mathbf{w}|\\theta,\\phi)\n",
    "=\n",
    "\\prod_{mn} \\text{Cat}(z_{mn}|\\theta_m) \n",
    "\\text{Cat}(w_{mn}|\\phi_{z_{mn}})\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\partial_{\\theta_m} \\log P(\\mathbf{z},\\mathbf{w}|\\theta,\\phi)\n",
    "=\n",
    "n_m/\\theta_m\n",
    "$$\n",
    "$$\n",
    "\\partial_{\\phi_k} \\log P(\\mathbf{z},\\mathbf{w}|\\theta,\\phi)\n",
    "=\n",
    "n_k/\\phi_k\n",
    "$$\n",
    "\n",
    "Excisie:\n",
    "Note the normalized pdf of $\\mathbf{z}$ and $\\mathbf{w}$ should be\n",
    "$$\n",
    "P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)\n",
    " =\n",
    "P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)/P(\\theta, \\phi)\n",
    " =\n",
    "P(\\theta, \\phi | \\mathbf{z}, \\mathbf{w}) P(\\mathbf{z}, \\mathbf{w})/P(\\theta, \\phi)\n",
    "$$\n",
    "\n",
    "we just recall\n",
    "$$\n",
    "P(\\theta, \\phi)\n",
    "=\n",
    "\\left[\\prod_k \\text{Dir} (\\phi_k|\\beta)\\right]\n",
    "\\left[\\prod_m \\text{Dir}(\\theta_m|\\alpha)\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log P(\\mathbf{z},\\mathbf{w}|\\theta,\\phi)\n",
    "= \n",
    "\\log P(\\mathbf{z}, \\mathbf{w})\n",
    "+ \\log \\left[\\prod_m \\text{Dir}(\\theta_m|n_m+\\alpha)\\right]\n",
    "+\\log\\left[\\prod_k \\text{Dir}(\\phi_k|n_k+\\beta)\\right]\n",
    "-\\log \\left[\\prod_m \\text{Dir}(\\theta_m|\\alpha)\\right]\n",
    "-\\log \\left[\\prod_k \\text{Dir} (\\phi_k|\\beta)\\right]\n",
    "$$\n",
    "we can again obtain the $P(\\mathbf{z}, \\mathbf{w}|\\theta, \\phi)$\n",
    "\n",
    "and \n",
    "$$\n",
    "P(\\mathbf{z} |\\mathbf{w}, \\theta, \\phi)\n",
    "=\n",
    "P(\\mathbf{z}, \\mathbf{w} | \\theta, \\phi)/ P(\\mathbf{w}|\\theta, \\phi)\n",
    "\\propto\n",
    "\\theta_{mk} \\phi_{kn}\n",
    "$$\n",
    "for the document $m$ and word $n$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### the LDA\n",
    "Like EM algoirhtm, we define the Q function as\n",
    "$$\n",
    "\\sum_{z} P(\\mathbf{z}, \\mathbf{w}, \\theta^i, \\phi^i) \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)\n",
    "$$\n",
    "We to maximize to Q (with constraint of $\\sum_k \\theta_{mk}=1$, or normalize in time)\n",
    "$$\n",
    "\\partial_\\theta \\sum_{z}  P(\\mathbf{z}, \\mathbf{w}, \\theta^i, \\phi^i) \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi) = 0\n",
    "$$\n",
    "$$\n",
    "\\sum_{z}  P(\\mathbf{z}, \\mathbf{w}, \\theta^i, \\phi^i) \\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "We can sample $z$ with probability of $ P(\\mathbf{z}, \\mathbf{w}, \\theta^i, \\phi^i)$, and move $\\theta$ and $\\phi$ along the direction of \n",
    "$-\\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)$ and $-\\partial_\\phi \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)$, respectively.\n",
    "The update ruler\n",
    "$$\n",
    "\\theta^{i+1} = \\theta + \\eta \\partial_\\theta \\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)\n",
    "$$\n",
    "$$\n",
    "\\phi^{i+1} = \\phi + \\eta \\partial_\\phi\\log  P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)\n",
    "$$\n",
    "\n",
    "It's simple that\n",
    "$$\n",
    "\\partial_{\\theta_m} \\log P(\\mathbf{z},\\mathbf{w}, \\theta,\\phi)\n",
    "=\n",
    "(n_m + \\alpha - 1)/\\theta_m\n",
    "$$\n",
    "$$\n",
    "\\partial_{\\phi_k} \\log P(\\mathbf{z},\\mathbf{w}, \\theta,\\phi)\n",
    "=\n",
    "(n_k + \\beta - 1)/\\phi_k\n",
    "$$\n",
    "\n",
    "and \n",
    "$$\n",
    "P(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi)\n",
    "\\propto\n",
    "\\theta_{mk} \\phi_{kn}\n",
    "$$\n",
    "for the document $m$ and word $n$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc-topic matrix\n",
      "                            topic0      topic1\n",
      "`       girl love boy'   +0.000000   +1.000000\n",
      "`       boy love girl'   +0.000000   +1.000000\n",
      "`      girl love girl'   +0.000000   +1.000000\n",
      "`        boy love boy'   +0.000000   +1.000000\n",
      "`   meson hits photon'   +1.000000   +0.000000\n",
      "`   photon hits meson'   +1.000000   +0.000000\n",
      "`  photon hits photon'   +1.000000   +0.000000\n",
      "`    meson hits meson'   +1.000000   +0.000000\n",
      "`      boy love meson'   +0.000000   +1.000000\n",
      "`    girl love photon'   +0.414214   +0.585786\n",
      "`     meson hits girl'   +1.000000   +0.000000\n",
      "`     photon hits boy'   +0.585786   +0.414214\n",
      "term-topic matrix\n",
      "                            topic0      topic1\n",
      "                 girl    +0.274868   +0.122925\n",
      "                 love    +0.122925   +0.274868\n",
      "                  boy    +0.301103   +0.000000\n",
      "                meson    +0.000000   +0.301103\n",
      "                 hits    +0.301103   +0.000000\n",
      "               photon    +0.000000   +0.301103\n"
     ]
    }
   ],
   "source": [
    "alpha=1.0\n",
    "beta=1.0\n",
    "\n",
    "theta_mk = np.random.rand(len(documents), ntopics)\n",
    "theta_mk = theta_mk / theta_mk.sum(axis=1, keepdims=True)\n",
    "phi_kv = np.random.rand(ntopics, len(terms_vec))\n",
    "phi_kv = phi_kv / phi_kv.sum(axis=1, keepdims=True)\n",
    "\n",
    "def zwtp_Gibbs_topic_prob(m, v):    \n",
    "    return theta_mk[m,:] * phi_kv[:,v]\n",
    "\n",
    "def zwtp_MH_topic_prob(m, v, k):\n",
    "    return theta_mk[m, k] * phi_kv[k, v]\n",
    "\n",
    "intialize_randomly()\n",
    "\n",
    "# warnming\n",
    "for _ in range(1000):\n",
    "    Gibbs_step(zwtp_Gibbs_topic_prob)\n",
    "\n",
    "def normalize(mat):\n",
    "    return mat/mat.sum(axis=1, keepdims=True)\n",
    "    \n",
    "eta = 0.01\n",
    "for _ in range(1000):\n",
    "    MH_step(zwtp_MH_topic_prob)\n",
    "    \n",
    "    #print(n_mk[:,:] + alpha - 1)\n",
    "    #print(theta_mk)\n",
    "    d = (n_mk[:,:] + alpha - 1)/theta_mk[:,:]\n",
    "    theta_mk = theta_mk + eta*d\n",
    "    theta_mk = normalize(theta_mk)\n",
    "    \n",
    "    d = (n_kv[:,:] + beta - 1)/phi_kv[:,:]\n",
    "    phi_kv = phi_kv + eta*d\n",
    "    phi_kv = normalize(phi_kv)\n",
    "    \n",
    "    #print(theta_mk)\n",
    "    #print(phi_kv)\n",
    "    #print_mat(n_kv.transpose(), n_mk.transpose())\n",
    "    #print(theta_mk.sum(axis=1))\n",
    "    #print(phi_kv.sum(axis=1))\n",
    "    #print(k_mn)\n",
    "    #print(n_kv)\n",
    "    #print(n_mk)\n",
    "    \n",
    "print_mat(phi_kv.transpose(), theta_mk.transpose())"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "24",
    "lenType": "16",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
